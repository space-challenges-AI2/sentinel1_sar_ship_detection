# YOLOv5 ðŸš€ by Ultralytics, GPL-3.0 license
"""
Common modules
"""

import ast
import contextlib
import json
import math
import platform
import warnings
import zipfile
from collections import OrderedDict, namedtuple
from copy import copy
from pathlib import Path
from urllib.parse import urlparse

import cv2
import numpy as np
import pandas as pd
import requests
import torch
import torch.nn as nn
from IPython.display import display
from PIL import Image
from torch.cuda import amp

from utils import TryExcept
from utils.dataloaders import exif_transpose, letterbox
from utils.general import (LOGGER, ROOT, Profile, check_requirements, check_suffix, check_version, colorstr,
                           increment_path, is_notebook, make_divisible, non_max_suppression, scale_boxes, xywh2xyxy,
                           xyxy2xywh, yaml_load)
from utils.plots import Annotator, colors, save_one_box
from utils.torch_utils import copy_attr, smart_inference_mode


def autopad(k, p=None, d=1):  # Define a function named autopad that takes three parameters: kernel (k), padding (p), and dilation (d)
    # Used to calculate the padding size that keeps the convolution kernel output shape unchanged in neural networks
    if d > 1:
        # If dilation (d) is greater than 1, calculate the actual kernel size
        k = d * (k - 1) + 1 if isinstance(k, int) else [d * (x - 1) + 1 for x in k]  # Calculate actual convolution kernel size
    if p is None:
        # If padding (p) is not specified, automatically calculate the padding size
        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # Automatically calculate padding size to keep output shape unchanged
    return p  # Return the calculated padding size


class Conv(nn.Module):  # Define a class named Conv that inherits from PyTorch's nn.Module
    # Standard convolution layer, parameters include input channels (ch_in), output channels (ch_out), kernel size (kernel), stride, padding, groups, dilation, activation function (activation)
    default_act = nn.SiLU()  # Default activation function is SiLU

    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):
        # Initialization function, set convolution layer parameters
        super().__init__()  # Call parent class initialization function
        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)
        # Create a 2D convolution layer, including input channels, output channels, kernel size, stride, auto-padding, groups, dilation and no bias term
        self.bn = nn.BatchNorm2d(c2)  # Create a batch normalization layer
        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()
        # Set activation function, if act is True use default activation function, if act is nn.Module instance use act, otherwise no activation function (identity mapping)

    def forward(self, x):
        # Forward propagation function
        return self.act(self.bn(self.conv(x)))  # Pass input x through convolution layer, batch normalization layer, then apply activation function

    def forward_fuse(self, x):
        # Another forward propagation function, used for fusing convolution layer and activation function
        return self.act(self.conv(x))  # Pass input x through convolution layer, then apply activation function



class DWConv(Conv):  # Define a class named DWConv that inherits from the previously defined Conv class
    # Depthwise separable convolution layer
    def __init__(self, c1, c2, k=1, s=1, d=1, act=True):  # Constructor, parameters include input channels (c1), output channels (c2), kernel size (k), stride (s), dilation (d), activation function (act)
        super().__init__(c1, c2, k, s, g=math.gcd(c1, c2), d=d, act=act)
        # Call parent class Conv constructor
        # Use math.gcd(c1, c2) to determine group count (g), where g is the greatest common divisor of input and output channel numbers
        # Other parameters are passed to parent class constructor
'''
`DWConv` (Depthwise Separable Convolution) and `Conv` (Standard Convolution) are two different convolution methods with significant differences in structure and computational efficiency:

1. **Standard Convolution (`Conv`)**:
   - In standard convolution, each output channel is generated by convolution kernels on all input channels.
   - For example, if there are 32 input channels and 64 output channels, each output channel's convolution kernel will be applied on all 32 input channels.
   - This method is very effective in feature extraction, but it involves extensive computation because each output channel needs to perform convolution operations with each input channel.

2. **Depthwise Separable Convolution (`DWConv`)**:
   - Depthwise separable convolution decomposes standard convolution into two steps: depthwise convolution (Depthwise Convolution) and pointwise convolution (Pointwise Convolution).
   - **Depthwise Convolution**: In the depthwise convolution step, a convolution kernel is applied separately to each input channel. This means each input channel only operates with its own convolution kernel without interacting with convolution kernels from other input channels.
   - **Pointwise Convolution**: Next, pointwise convolution uses 1x1 convolution kernels to combine the outputs of depthwise convolution, generating the final output channels.
   - This method significantly reduces computational load and parameter count because depthwise convolution doesn't need to perform fully connected convolution operations on all input channels.

In summary, `DWConv` effectively reduces computational complexity and model size by decomposing the convolution process while maintaining sufficient feature extraction capability.
This makes it very useful in designing lightweight and efficient convolutional neural network architectures, especially in resource-constrained environments (such as mobile devices).
In contrast, `Conv` provides more comprehensive feature extraction capability, but at the cost of higher computational and parameter overhead.
'''


class DWConvTranspose2d(nn.ConvTranspose2d):  # Define a class named DWConvTranspose2d that inherits from PyTorch's nn.ConvTranspose2d
    # Depthwise separable transposed convolution layer
    def __init__(self, c1, c2, k=1, s=1, p1=0, p2=0):  # Constructor, parameters include input channels (c1), output channels (c2), kernel size (k), stride (s), input padding (p1), output padding (p2)
        super().__init__(c1, c2, k, s, p1, p2, groups=math.gcd(c1, c2))
        # Call parent class nn.ConvTranspose2d constructor
        # Set group count to the greatest common divisor of input and output channel numbers to achieve depthwise convolution effect
'''
Transposed convolution (sometimes also called deconvolution or inverse convolution) is commonly used for upsampling operations in convolutional neural networks. Its function is to enlarge the input feature map to higher spatial resolution.
'''


class TransformerLayer(nn.Module):  # Define a class named TransformerLayer that inherits from PyTorch's nn.Module
    # Transformer layer implementation (removed LayerNorm layer to improve performance)

    def __init__(self, c, num_heads):
        # Initialization function, parameters include channel count (c) and number of attention heads (num_heads)
        super().__init__()  # Call parent class initialization function
        self.q = nn.Linear(c, c, bias=False)  # Define query (Query) linear transformation layer
        self.k = nn.Linear(c, c, bias=False)  # Define key (Key) linear transformation layer
        self.v = nn.Linear(c, c, bias=False)  # Define value (Value) linear transformation layer
        self.ma = nn.MultiheadAttention(embed_dim=c, num_heads=num_heads)  # Define multi-head self-attention layer
        self.fc1 = nn.Linear(c, c, bias=False)  # Define first linear layer in feedforward network
        self.fc2 = nn.Linear(c, c, bias=False)  # Define second linear layer in feedforward network

    def forward(self, x):
        # Forward propagation function
        x = self.ma(self.q(x), self.k(x), self.v(x))[0] + x  # Pass input x through q, k, v linear layers, then into multi-head self-attention layer, and add to original input (residual connection)
        x = self.fc2(self.fc1(x)) + x  # Pass self-attention output through feedforward network, and add to self-attention layer output (residual connection)
        return x  # Return final output
'''
The combination of these layers provides two key capabilities of the Transformer architecture: self-attention and feedforward network processing.
Self-attention allows the model to focus on different parts of the input and learn relationships between them, while the feedforward network further processes this information, allowing for more complex data representations.
Residual connections maintain information flow throughout the process and help alleviate gradient vanishing problems in deep networks.
'''


class TransformerBlock(nn.Module):  # Define a class named TransformerBlock that inherits from PyTorch's nn.Module
    # Vision Transformer implementation, reference paper: https://arxiv.org/abs/2010.11929

    def __init__(self, c1, c2, num_heads, num_layers):
        # Initialization function, parameters include input channels (c1), output channels (c2), number of attention heads (num_heads) and number of Transformer layers (num_layers)
        super().__init__()  # Call parent class initialization function
        self.conv = None  # Initialize convolution layer as None
        if c1 != c2:
            self.conv = Conv(c1, c2)  # If input and output channel counts are different, use convolution layer to adjust channel count
        self.linear = nn.Linear(c2, c2)  # Define a linear layer for learning position embeddings
        self.tr = nn.Sequential(*(TransformerLayer(c2, num_heads) for _ in range(num_layers)))
        # Create a sequence of TransformerLayer instances, number specified by num_layers
        self.c2 = c2  # Store output channel count

    def forward(self, x):
        # Forward propagation function
        if self.conv is not None:
            x = self.conv(x)  # If convolution layer exists, apply it first
        b, _, w, h = x.shape  # Get batch size, channels, width, height
        p = x.flatten(2).unsqueeze(0).transpose(0, 3).squeeze(3)  # Flatten spatial dimensions and prepare for transformer
        return self.tr(p + self.linear(p)).unsqueeze(3).transpose(0, 3).reshape(b, self.c2, w, h)  # Apply transformer and reshape back

'''
'''

class Bottleneck(nn.Module):  # Define a class named Bottleneck that inherits from PyTorch's nn.Module
    # Standard bottleneck structure
    def __init__(self, c1, c2, shortcut=True, g=1, e=0.5):  # Constructor, parameters include input channels (c1), output channels (c2), whether to use shortcut, group count (g), expansion factor (e)
        super().__init__()  # Call parent class initialization function
        c_ = int(c2 * e)  # Calculate hidden layer channel count, which is the product of output channels and expansion factor
        self.cv1 = Conv(c1, c_, 1, 1)  # First convolution layer, using 1x1 kernel, for dimension reduction
        self.cv2 = Conv(c_, c2, 3, 1, g=g)  # Second convolution layer, using 3x3 kernel, restore dimensions, can have grouping
        self.add = shortcut and c1 == c2  # Whether to add shortcut connection, only if shortcut is True and input/output channels are equal

    def forward(self, x):
        # Forward propagation function
        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))  # If shortcut is enabled, add input to output; otherwise just return the processed output



class BottleneckCSP(nn.Module):  # Define a class named BottleneckCSP that inherits from PyTorch's nn.Module
    # CSP bottleneck structure, derived from Cross Stage Partial Networks
    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # Constructor, parameters include input channels (c1), output channels (c2), number of Bottleneck layers (n), whether to use shortcut, group count (g), expansion factor (e)
        super().__init__()  # Call parent class initialization function
        c_ = int(c2 * e)  # Calculate hidden layer channel count
        self.cv1 = Conv(c1, c_, 1, 1)  # First convolution layer, 1x1 convolution, for dimension reduction
        self.cv2 = nn.Conv2d(c1, c_, 1, 1, bias=False)  # Second convolution layer, 1x1 convolution, also for dimension reduction
        self.cv3 = nn.Conv2d(c_, c_, 1, 1, bias=False)  # Third convolution layer, 1x1 convolution, for processing features through Bottleneck
        self.cv4 = Conv(2 * c_, c2, 1, 1)  # Fourth convolution layer, 1x1 convolution, for combining features
        self.bn = nn.BatchNorm2d(2 * c_)  # Batch normalization layer
        self.act = nn.SiLU()  # Activation function
        self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)))  # Create a sequence of Bottleneck layers

    def forward(self, x):
        # Forward propagation function
        y1 = self.cv3(self.m(self.cv1(x)))  # Process through first path: cv1 -> m -> cv3
        y2 = self.cv2(x)  # Process through second path: cv2
        return self.cv4(self.act(self.bn(torch.cat((y1, y2), dim=1))))  # Concatenate and process through final layers



class CrossConv(nn.Module):  # Define a class named CrossConv that inherits from PyTorch's nn.Module
    # Cross convolution downsampling
    def __init__(self, c1, c2, k=3, s=1, g=1, e=1.0, shortcut=False):
        # Constructor, parameters include input channels (c1), output channels (c2), kernel size (k), stride (s), group count (g), expansion factor (e), whether to use shortcut
        super().__init__()  # Call parent class initialization function
        c_ = int(c2 * e)  # Calculate hidden layer channel count
        self.cv1 = Conv(c1, c_, (1, k), (1, s))  # First convolution layer, using (1, k) kernel size, mainly convolves along one direction
        self.cv2 = Conv(c_, c2, (k, 1), (s, 1), g=g)  # Second convolution layer, using (k, 1) kernel size, convolves along direction perpendicular to first convolution layer
        self.add = shortcut and c1 == c2  # Whether to add shortcut connection

    def forward(self, x):
        # Forward propagation function
        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))  # If shortcut is enabled, add input to output; otherwise just return the processed output



class C3(nn.Module):  # Define a class named C3 that inherits from PyTorch's nn.Module
    # CSP bottleneck structure with three convolution layers
    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # Constructor, parameters include input channels (c1), output channels (c2), number of Bottleneck layers (n), whether to use shortcut, group count (g), expansion factor (e)
        super().__init__()  # Call parent class initialization function
        c_ = int(c2 * e)  # Calculate hidden layer channel count
        self.cv1 = Conv(c1, c_, 1, 1)  # First convolution layer, 1x1 convolution, for dimension reduction
        self.cv2 = Conv(c1, c_, 1, 1)  # Second convolution layer, 1x1 convolution, also for dimension reduction
        self.cv3 = Conv(2 * c_, c2, 1)  # Third convolution layer, 1x1 convolution, for combining features
        self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)))  # Create a sequence of Bottleneck layers

    def forward(self, x):
        # Forward propagation function
        return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), dim=1))  # Process through first path, concatenate with second path, then through final convolution



class C3x(C3):  # Define a class named C3x that inherits from C3 class
    # C3 module with cross convolution

    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):
        # Constructor, parameters are the same as C3 class
        super().__init__(c1, c2, n, shortcut, g, e)  # Call parent class C3 constructor
        c_ = int(c2 * e)  # Calculate hidden layer channel count, using expansion factor e
        # Use CrossConv instead of Bottleneck in C3 class, create a new nn.Sequential
        self.m = nn.Sequential(*(CrossConv(c_, c_, 3, 1, g, 1.0, shortcut) for _ in range(n)))



class C3TR(C3):  # Define a class named C3TR that inherits from C3 class
    # C3 module with TransformerBlock

    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):
        # Constructor, parameters include input channels (c1), output channels (c2), TransformerBlock layers (n), whether to use shortcut connection, group count (g), expansion factor (e)
        super().__init__(c1, c2, n, shortcut, g, e)  # Call parent class C3 constructor
        c_ = int(c2 * e)  # Calculate hidden layer channel count, which is output channels times expansion factor e
        # Use TransformerBlock instead of Bottleneck in C3 class, create a TransformerBlock instance
        self.m = TransformerBlock(c_, c_, 4, n)  # Parameter 4 represents the number of heads in TransformerBlock



class C3SPP(C3):  # Define a class named C3SPP that inherits from C3 class
    # C3 module with SPP (Spatial Pyramid Pooling)

    def __init__(self, c1, c2, k=(5, 9, 13), n=1, shortcut=True, g=1, e=0.5):
        # Constructor, parameters include input channels (c1), output channels (c2), list of SPP kernel sizes (k), number of layers (n), whether to use shortcut connection, group count (g), and expansion factor (e)
        super().__init__(c1, c2, n, shortcut, g, e)  # Call parent class C3 constructor
        c_ = int(c2 * e)  # Calculate hidden layer channel count, which is output channels times expansion factor e
        # Replace Bottleneck in C3 class with SPP module to enhance feature extraction
        self.m = SPP(c_, c_, k)  # Use given list of kernel sizes k to initialize SPP module



class C3Ghost(C3):  # Define a class named C3Ghost that inherits from C3 class
    # C3 module with GhostBottleneck

    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):
        # Constructor, parameters include input channels (c1), output channels (c2), GhostBottleneck layers (n), whether to use shortcut connection, group count (g), and expansion factor (e)
        super().__init__(c1, c2, n, shortcut, g, e)  # Call parent class C3 constructor
        c_ = int(c2 * e)  # Calculate hidden layer channel count, which is output channels times expansion factor e
        # Use nn.Sequential to create a sequence containing n GhostBottleneck modules
        self.m = nn.Sequential(*(GhostBottleneck(c_, c_) for _ in range(n)))



class SPP(nn.Module):  # Define a class named SPP that inherits from PyTorch's nn.Module
    # Spatial Pyramid Pooling (SPP) layer

    def __init__(self, c1, c2, k=(5, 9, 13)):
        # Constructor, parameters include input channels (c1), output channels (c2), and a list of pooling kernel sizes (k)
        super().__init__()  # Call parent class initialization function
        c_ = c1 // 2  # Calculate hidden layer channel count, which is half of input channels
        self.cv1 = Conv(c1, c_, 1, 1)  # First convolution layer, using 1x1 convolution kernel, for dimension reduction
        self.cv2 = Conv(c_ * (len(k) + 1), c2, 1, 1)
        # Second convolution layer, used to merge features from SPP layers and adjust channel count
        self.m = nn.ModuleList([nn.MaxPool2d(kernel_size=x, stride=1, padding=x // 2) for x in k])
        # Create a module list containing max pooling layers of different sizes

    def forward(self, x):
        # Forward propagation function
        x = self.cv1(x)  # Pass through first convolution layer
        with warnings.catch_warnings():
            warnings.simplefilter('ignore')  # Ignore warning for torch 1.9.0 max_pool2d()
            # Pass through a series of max pooling layers, then concatenate with original feature map
            return self.cv2(torch.cat([x] + [m(x) for m in self.m], 1))
            # Pass through second convolution layer, merge features and adjust channel count
'''
SPP layer can capture spatial information at different scales, which is very useful for tasks with varying scales.
By pooling at different scales, SPP layer can enhance the model's adaptability to scale changes.
'''


class SPPF(nn.Module):  # Define a class named SPPF that inherits from PyTorch's nn.Module
    # Fast Spatial Pyramid Pooling (SPPF) layer, used for YOLOv5

    def __init__(self, c1, c2, k=5):  # Constructor, parameters include input channels (c1), output channels (c2), and pooling kernel size (k)
        super().__init__()  # Call parent class initialization function
        c_ = c1 // 2  # Calculate hidden layer channel count, which is half of input channels
        self.cv1 = Conv(c1, c_, 1, 1)  # First convolution layer, using 1x1 convolution kernel, for dimension reduction
        self.cv2 = Conv(c_ * 4, c2, 1, 1)  # Second convolution layer, used to merge features and adjust channel count
        self.m = nn.MaxPool2d(kernel_size=k, stride=1, padding=k // 2)
        # Define a max pooling layer with given kernel size k

    def forward(self, x):
        # Forward propagation function
        x = self.cv1(x)  # Pass through first convolution layer
        with warnings.catch_warnings():
            warnings.simplefilter('ignore')  # Ignore warning for torch 1.9.0 max_pool2d()
            y1 = self.m(x)  # Apply max pooling once
            y2 = self.m(y1)  # Apply max pooling again to y1
            # Concatenate original feature map x with results of y1, y2, and y2 after max pooling
            return self.cv2(torch.cat((x, y1, y2, self.m(y2)), 1))
            # Pass through second convolution layer, merge features and adjust channel count
'''
This method is more efficient than traditional SPP layer (using multiple pooling layers of different sizes) because it only uses a single pooling layer applied multiple times to the input.
This design retains the ability to capture multi-scale spatial information while boosting efficiency, making it particularly suitable for real-time object detection tasks.
'''


class Focus(nn.Module):  # Define a class named Focus that inherits from PyTorch's nn.Module
    # Focus on spatial information of image width and height to channel space

    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # Constructor, parameters include input channels (c1), output channels (c2), convolution kernel size (k), stride (s), padding (p), group count (g), and whether to use activation function (act)
        super().__init__()  # Call parent class initialization function
        self.conv = Conv(c1 * 4, c2, k, s, p, g, act=act)  # Initialize convolution layer, multiply input channels by 4

    def forward(self, x):  # x shape is (b, c, w, h)
        # Rearrange sub-pixels of input tensor and concatenate along channel dimension
        return self.conv(torch.cat((x[..., ::2, ::2], x[..., 1::2, ::2], x[..., ::2, 1::2], x[..., 1::2, 1::2]), 1))
        # Apply convolution to concatenated tensor
'''
Focus layer is designed to effectively focus spatial information of image to channel dimension while reducing spatial dimensions of input data (width and height halved), which is beneficial for increasing network's computational efficiency and performance.
By showing an example of how Focus layer works. Suppose we have a simple input tensor x, whose shape is [1, 2, 4, 4], where 1 is batch size, 2 is number of channels, and 4x4 is width and height. We will use Focus layer to process this tensor and observe the output.

Suppose input tensor x is as follows (for simplification, I used smaller numbers):

```
Tensor x shape: [1, 2, 4, 4]
Content (assumed):
First channel:    Second channel:
1 2 3 4           5 6 7 8
5 6 7 8           1 2 3 4
9 0 1 2           9 0 1 2
3 4 5 6           3 4 5 6
```

Focus layer will perform the following operations:

1. Divide input tensor x into 2x2 blocks, and rearrange elements of each block to channel dimension. This will increase the number of channels while reducing the size of width and height.

   - Extract top-left elements of each 2x2 block (`::2, ::2`).
   - Extract top-right elements of each 2x2 block (`1::2, ::2`).
   - Extract bottom-left elements of each 2x2 block (`::2, 1::2`).
   - Extract bottom-right elements of each 2x2 block (`1::2, 1::2`).

2. Concatenate these extracted elements along channel dimension to form a new tensor.

Result is that the number of channels increases by 4 (from 2 to 8 in this example), while width and height are halved (from 4x4 to 2x2).

Finally, output tensor of Focus layer may look like this (assuming no further convolution processing):

```
Output tensor shape: [1, 8, 2, 2]
Content (rearranged according to above steps):
Channels 1-4:        Channels 5-8:
1 3   2 4           5 7   6 8
9 1   0 2           9 1   0 2

5 7   6 8           1 3   2 4
3 5   4 6           3 5   4 6
```

This operation effectively focuses spatial information "on" channel dimension while reducing spatial dimensions of data, which is beneficial for reducing computational load and improving efficiency in many deep learning models.
Especially useful in processing image data, this method can help the model more effectively capture and utilize spatial information.
'''


class GhostConv(nn.Module):  # Define a class named GhostConv that inherits from PyTorch's nn.Module
    # Ghost convolution

    def __init__(self, c1, c2, k=1, s=1, g=1, act=True):  # Constructor, parameters include input channels (c1), output channels (c2), convolution kernel size (k), stride (s), group count (g), and whether to use activation function (act)
        super().__init__()  # Call parent class initialization function
        c_ = c2 // 2  # Calculate hidden layer channel count, which is half of output channels
        self.cv1 = Conv(c1, c_, k, s, None, g, act=act)  # First convolution layer, responsible for generating half of feature map
        self.cv2 = Conv(c_, c_, 5, 1, None, c_, act=act)  # Second convolution layer, using larger convolution kernel to generate remaining feature map

    def forward(self, x):
        # Forward propagation function
        y = self.cv1(x)  # Pass through first convolution layer to generate half of feature map
        return torch.cat((y, self.cv2(y)), 1)  # Concatenate output of first convolution layer with second convolution layer along channel dimension
'''
Ghost convolution (Ghost Convolution) is an efficient convolution method that aims to reduce the computational load of traditional convolution operations without sacrificing network performance.
This method was initially proposed in [GhostNet](https://github.com/huawei-noah/ghostnet) research, mainly used for mobile and computing efficiency-sensitive applications.
The core purpose and principle of Ghost convolution are as follows:

### Core purpose:

1. **Reduce computational load**: Ghost convolution reduces the computational load of traditional convolution operations by reducing the number of convolution kernels.
2. **Maintain performance**: Although the computational load is reduced, Ghost convolution can maintain a similar performance level through clever design.

### Principle:

Ghost convolution is based on the idea of decomposing traditional convolution operations into two steps:

1. **Original convolution**: First, use fewer convolution kernels to convolve the input feature map, generating part of the feature map.
This step is similar to conventional convolution, but the number of convolution kernels used is much less than traditional convolution.

2. **Cheap operation**: Then, apply a cheap linear operation (such as linear convolution, depthwise convolution, or other cheap transformations) to the generated feature map.
This step does not involve extensive multiplication operations, so the computational cost is low.

### Result:

The total number of feature maps generated by this method is the same as traditional convolution, but because the second step uses a low-cost operation, the overall computational load is reduced. Ghost convolution is particularly suitable for scenarios requiring lightweight network design, such as mobile devices, edge computing, and real-time applications.

In summary, the purpose of Ghost convolution is to maintain network performance while reducing the computational complexity and number of parameters of the model, thereby improving efficiency and speed.
'''


class GhostBottleneck(nn.Module):  # Define a class named GhostBottleneck that inherits from PyTorch's nn.Module
    # Ghost bottleneck structure

    def __init__(self, c1, c2, k=3, s=1):  # Constructor, parameters include input channels (c1), output channels (c2), convolution kernel size (k), stride (s)
        super().__init__()  # Call parent class initialization function
        c_ = c2 // 2  # Calculate hidden layer channel count, which is half of output channels
        self.conv = nn.Sequential(
            GhostConv(c1, c_, 1, 1),  # First Ghost convolution layer, used for point-wise convolution (pw)
            DWConv(c_, c_, k, s, act=False) if s == 2 else nn.Identity(),  # Depthwise separable convolution layer (dw), used when stride is 2
            GhostConv(c_, c2, 1, 1, act=False))  # Second Ghost convolution layer, used for linear transformation
        self.shortcut = nn.Sequential(
            DWConv(c1, c1, k, s, act=False),
            Conv(c1, c2, 1, 1, act=False)
        ) if s == 2 else nn.Identity()  # shortcut path, includes depthwise separable convolution and 1x1 convolution when stride is 2

    def forward(self, x):
        # Forward propagation function
        return self.conv(x) + self.shortcut(x)  # Add output of conv path and shortcut path



class Contract(nn.Module):  # Define a class named Contract that inherits from PyTorch's nn.Module
    # Compress width and height information to channel dimension

    def __init__(self, gain=2):
        # Constructor, parameter gain represents the fold of spatial dimension reduction
        super().__init__()  # Call parent class initialization function
        self.gain = gain  # Set the fold of spatial dimension reduction

    def forward(self, x):
        # Forward propagation function
        b, c, h, w = x.size()  # Get dimensions of input tensor: batch size (b), number of channels (c), height (h), width (w)
        s = self.gain  # Get the fold of spatial compression
        x = x.view(b, c, h // s, s, w // s, s)  # Rearrange tensor, prepare for compression
        x = x.permute(0, 3, 5, 1, 2, 4).contiguous()  # Change the order of dimensions of tensor to (b, s, s, c, h//s, w//s)
        return x.view(b, c * s * s, h // s, w // s)  # Compress tensor to new shape (b, c * s * s, h//s, w//s)



class Expand(nn.Module):  # Define a class named Expand that inherits from PyTorch's nn.Module
    # Expand channels to width and height

    def __init__(self, gain=2):
        # Constructor, parameter gain represents the fold of spatial dimension increase
        super().__init__()  # Call parent class initialization function
        self.gain = gain  # Set the fold of spatial dimension increase

    def forward(self, x):
        # Forward propagation function
        b, c, h, w = x.size()  # Get dimensions of input tensor: batch size (b), number of channels (c), height (h), width (w)
        s = self.gain  # Get the fold of spatial expansion
        x = x.view(b, s, s, c // s ** 2, h, w)  # Rearrange tensor, prepare for expansion
        x = x.permute(0, 3, 4, 1, 5, 2).contiguous()  # Change the order of dimensions of tensor to (b, c // s ** 2, h, s, w, s)
        return x.view(b, c // s ** 2, h * s, w * s)  # Expand tensor to new shape (b, c // s ** 2, h * s, w * s)



class Concat(nn.Module):
    # Concatenate a list of tensors along dimension
    def __init__(self, dimension=1):
        super().__init__()
        self.d = dimension

    def forward(self, x):
        return torch.cat(x, self.d)


'''
DetectMultiBackend

1. **Multi-backend detection**: The name "Detect" in the function implies that it is related to object detection tasks. "MultiBackend" indicates that it can support multiple backends.
In the field of machine learning and computer vision, "backend" usually refers to the library or framework that executes calculations, such as PyTorch, TensorFlow, OpenCV, etc.

2. **Framework compatibility**: This function may be designed to execute similar object detection tasks in different computing frameworks or libraries, automatically selecting or compatible with multiple backends.

3. **Model and hardware adaptation**: It may contain logic to handle different types of models (e.g., different neural network architectures) and may consider the hardware environment of running models (e.g., CPU, GPU, or specific hardware accelerators).

4. **Automation and optimization**: The function may include automatic selection of the optimal backend based on the current environment and available resources (e.g., memory and computing power) to optimize performance.

5. **Uniform interface**: To handle multiple backends, this function may provide a uniform interface that allows users to execute detection tasks without having to care about low-level details.

In summary, `DetectMultiBackend` is a general, flexible function designed to execute object detection tasks in different technology stacks and hardware environments.
Such design allows it to adapt to various application scenarios and performance requirements.
'''
class DetectMultiBackend(nn.Module):
    # YOLOv5 MultiBackend class for python inference on various backends
    def __init__(self, weights='yolov5s.pt', device=torch.device('cpu'), dnn=False, data=None, fp16=False, fuse=True):
        # Usage:
        #   PyTorch:              weights = *.pt
        #   TorchScript:                    *.torchscript
        #   ONNX Runtime:                   *.onnx
        #   ONNX OpenCV DNN:                *.onnx --dnn
        #   OpenVINO:                       *_openvino_model
        #   CoreML:                         *.mlmodel
        #   TensorRT:                       *.engine
        #   TensorFlow SavedModel:          *_saved_model
        #   TensorFlow GraphDef:            *.pb
        #   TensorFlow Lite:                *.tflite
        #   TensorFlow Edge TPU:            *_edgetpu.tflite
        #   PaddlePaddle:                   *_paddle_model
        from models.experimental import attempt_download, attempt_load  # scoped to avoid circular import

        super().__init__()
        w = str(weights[0] if isinstance(weights, list) else weights)
        pt, jit, onnx, xml, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs, paddle, triton = self._model_type(w)
        fp16 &= pt or jit or onnx or engine  # FP16
        nhwc = coreml or saved_model or pb or tflite or edgetpu  # BHWC formats (vs torch BCWH)
        stride = 32  # default stride
        cuda = torch.cuda.is_available() and device.type != 'cpu'  # use CUDA
        if not (pt or triton):
            w = attempt_download(w)  # download if not local

        if pt:  # PyTorch
            model = attempt_load(weights if isinstance(weights, list) else w, device=device, inplace=True, fuse=fuse)
            stride = max(int(model.stride.max()), 32)  # model stride
            names = model.module.names if hasattr(model, 'module') else model.names  # get class names
            model.half() if fp16 else model.float()
            self.model = model  # explicitly assign for to(), cpu(), cuda(), half()
        elif jit:  # TorchScript
            LOGGER.info(f'Loading {w} for TorchScript inference...')
            extra_files = {'config.txt': ''}  # model metadata
            model = torch.jit.load(w, _extra_files=extra_files, map_location=device)
            model.half() if fp16 else model.float()
            if extra_files['config.txt']:  # load metadata dict
                d = json.loads(extra_files['config.txt'],
                               object_hook=lambda d: {int(k) if k.isdigit() else k: v
                                                      for k, v in d.items()})
                stride, names = int(d['stride']), d['names']
        elif dnn:  # ONNX OpenCV DNN
            LOGGER.info(f'Loading {w} for ONNX OpenCV DNN inference...')
            check_requirements('opencv-python>=4.5.4')
            net = cv2.dnn.readNetFromONNX(w)
        elif onnx:  # ONNX Runtime
            LOGGER.info(f'Loading {w} for ONNX Runtime inference...')
            check_requirements(('onnx', 'onnxruntime-gpu' if cuda else 'onnxruntime'))
            import onnxruntime
            providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if cuda else ['CPUExecutionProvider']
            session = onnxruntime.InferenceSession(w, providers=providers)
            output_names = [x.name for x in session.get_outputs()]
            meta = session.get_modelmeta().custom_metadata_map  # metadata
            if 'stride' in meta:
                stride, names = int(meta['stride']), eval(meta['names'])
        elif xml:  # OpenVINO
            LOGGER.info(f'Loading {w} for OpenVINO inference...')
            check_requirements('openvino')  # requires openvino-dev: https://pypi.org/project/openvino-dev/
            from openvino.runtime import Core, Layout, get_batch
            ie = Core()
            if not Path(w).is_file():  # if not *.xml
                w = next(Path(w).glob('*.xml'))  # get *.xml file from *_openvino_model dir
            network = ie.read_model(model=w, weights=Path(w).with_suffix('.bin'))
            if network.get_parameters()[0].get_layout().empty:
                network.get_parameters()[0].set_layout(Layout("NCHW"))
            batch_dim = get_batch(network)
            if batch_dim.is_static:
                batch_size = batch_dim.get_length()
            executable_network = ie.compile_model(network, device_name="CPU")  # device_name="MYRIAD" for Intel NCS2
            stride, names = self._load_metadata(Path(w).with_suffix('.yaml'))  # load metadata
        elif engine:  # TensorRT
            LOGGER.info(f'Loading {w} for TensorRT inference...')
            import tensorrt as trt  # https://developer.nvidia.com/nvidia-tensorrt-download
            check_version(trt.__version__, '7.0.0', hard=True)  # require tensorrt>=7.0.0
            if device.type == 'cpu':
                device = torch.device('cuda:0')
            Binding = namedtuple('Binding', ('name', 'dtype', 'shape', 'data', 'ptr'))
            logger = trt.Logger(trt.Logger.INFO)
            with open(w, 'rb') as f, trt.Runtime(logger) as runtime:
                model = runtime.deserialize_cuda_engine(f.read())
            context = model.create_execution_context()
            bindings = OrderedDict()
            output_names = []
            fp16 = False  # default updated below
            dynamic = False
            for i in range(model.num_bindings):
                name = model.get_binding_name(i)
                dtype = trt.nptype(model.get_binding_dtype(i))
                if model.binding_is_input(i):
                    if -1 in tuple(model.get_binding_shape(i)):  # dynamic
                        dynamic = True
                        context.set_binding_shape(i, tuple(model.get_profile_shape(0, i)[2]))
                    if dtype == np.float16:
                        fp16 = True
                else:  # output
                    output_names.append(name)
                shape = tuple(context.get_binding_shape(i))
                im = torch.from_numpy(np.empty(shape, dtype=dtype)).to(device)
                bindings[name] = Binding(name, dtype, shape, im, int(im.data_ptr()))
            binding_addrs = OrderedDict((n, d.ptr) for n, d in bindings.items())
            batch_size = bindings['images'].shape[0]  # if dynamic, this is instead max batch size
        elif coreml:  # CoreML
            LOGGER.info(f'Loading {w} for CoreML inference...')
            import coremltools as ct
            model = ct.models.MLModel(w)
        elif saved_model:  # TF SavedModel
            LOGGER.info(f'Loading {w} for TensorFlow SavedModel inference...')
            import tensorflow as tf
            keras = False  # assume TF1 saved_model
            model = tf.keras.models.load_model(w) if keras else tf.saved_model.load(w)
        elif pb:  # GraphDef https://www.tensorflow.org/guide/migrate#a_graphpb_or_graphpbtxt
            LOGGER.info(f'Loading {w} for TensorFlow GraphDef inference...')
            import tensorflow as tf

            def wrap_frozen_graph(gd, inputs, outputs):
                x = tf.compat.v1.wrap_function(lambda: tf.compat.v1.import_graph_def(gd, name=""), [])  # wrapped
                ge = x.graph.as_graph_element
                return x.prune(tf.nest.map_structure(ge, inputs), tf.nest.map_structure(ge, outputs))

            def gd_outputs(gd):
                name_list, input_list = [], []
                for node in gd.node:  # tensorflow.core.framework.node_def_pb2.NodeDef
                    name_list.append(node.name)
                    input_list.extend(node.input)
                return sorted(f'{x}:0' for x in list(set(name_list) - set(input_list)) if not x.startswith('NoOp'))

            gd = tf.Graph().as_graph_def()  # TF GraphDef
            with open(w, 'rb') as f:
                gd.ParseFromString(f.read())
            frozen_func = wrap_frozen_graph(gd, inputs="x:0", outputs=gd_outputs(gd))
        elif tflite or edgetpu:  # https://www.tensorflow.org/lite/guide/python#install_tensorflow_lite_for_python
            try:  # https://coral.ai/docs/edgetpu/tflite-python/#update-existing-tf-lite-code-for-the-edge-tpu
                from tflite_runtime.interpreter import Interpreter, load_delegate
            except ImportError:
                import tensorflow as tf
                Interpreter, load_delegate = tf.lite.Interpreter, tf.lite.experimental.load_delegate,
            if edgetpu:  # TF Edge TPU https://coral.ai/software/#edgetpu-runtime
                LOGGER.info(f'Loading {w} for TensorFlow Lite Edge TPU inference...')
                delegate = {
                    'Linux': 'libedgetpu.so.1',
                    'Darwin': 'libedgetpu.1.dylib',
                    'Windows': 'edgetpu.dll'}[platform.system()]
                interpreter = Interpreter(model_path=w, experimental_delegates=[load_delegate(delegate)])
            else:  # TFLite
                LOGGER.info(f'Loading {w} for TensorFlow Lite inference...')
                interpreter = Interpreter(model_path=w)  # load TFLite model
            interpreter.allocate_tensors()  # allocate
            input_details = interpreter.get_input_details()  # inputs
            output_details = interpreter.get_output_details()  # outputs
            # load metadata
            with contextlib.suppress(zipfile.BadZipFile):
                with zipfile.ZipFile(w, "r") as model:
                    meta_file = model.namelist()[0]
                    meta = ast.literal_eval(model.read(meta_file).decode("utf-8"))
                    stride, names = int(meta['stride']), meta['names']
        elif tfjs:  # TF.js
            raise NotImplementedError('ERROR: YOLOv5 TF.js inference is not supported')
        elif paddle:  # PaddlePaddle
            LOGGER.info(f'Loading {w} for PaddlePaddle inference...')
            check_requirements('paddlepaddle-gpu' if cuda else 'paddlepaddle')
            import paddle.inference as pdi
            if not Path(w).is_file():  # if not *.pdmodel
                w = next(Path(w).rglob('*.pdmodel'))  # get *.pdmodel file from *_paddle_model dir
            weights = Path(w).with_suffix('.pdiparams')
            config = pdi.Config(str(w), str(weights))
            if cuda:
                config.enable_use_gpu(memory_pool_init_size_mb=2048, device_id=0)
            predictor = pdi.create_predictor(config)
            input_handle = predictor.get_input_handle(predictor.get_input_names()[0])
            output_names = predictor.get_output_names()
        elif triton:  # NVIDIA Triton Inference Server
            LOGGER.info(f'Using {w} as Triton Inference Server...')
            check_requirements('tritonclient[all]')
            from utils.triton import TritonRemoteModel
            model = TritonRemoteModel(url=w)
            nhwc = model.runtime.startswith("tensorflow")
        else:
            raise NotImplementedError(f'ERROR: {w} is not a supported format')

        # class names
        if 'names' not in locals():
            names = yaml_load(data)['names'] if data else {i: f'class{i}' for i in range(999)}
        if names[0] == 'n01440764' and len(names) == 1000:  # ImageNet
            names = yaml_load(ROOT / 'data/ImageNet.yaml')['names']  # human-readable names

        self.__dict__.update(locals())  # assign all variables to self

    def forward(self, im, augment=False, visualize=False):
        # YOLOv5 MultiBackend inference
        b, ch, h, w = im.shape  # batch, channel, height, width
        if self.fp16 and im.dtype != torch.float16:
            im = im.half()  # to FP16
        if self.nhwc:
            im = im.permute(0, 2, 3, 1)  # torch BCHW to numpy BHWC shape(1,320,192,3)

        if self.pt:  # PyTorch
            y = self.model(im, augment=augment, visualize=visualize) if augment or visualize else self.model(im)
        elif self.jit:  # TorchScript
            y = self.model(im)
        elif self.dnn:  # ONNX OpenCV DNN
            im = im.cpu().numpy()  # torch to numpy
            self.net.setInput(im)
            y = self.net.forward()
        elif self.onnx:  # ONNX Runtime
            im = im.cpu().numpy()  # torch to numpy
            y = self.session.run(self.output_names, {self.session.get_inputs()[0].name: im})
        elif self.xml:  # OpenVINO
            im = im.cpu().numpy()  # FP32
            y = list(self.executable_network([im]).values())
        elif self.engine:  # TensorRT
            if self.dynamic and im.shape != self.bindings['images'].shape:
                i = self.model.get_binding_index('images')
                self.context.set_binding_shape(i, im.shape)  # reshape if dynamic
                self.bindings['images'] = self.bindings['images']._replace(shape=im.shape)
                for name in self.output_names:
                    i = self.model.get_binding_index(name)
                    self.bindings[name].data.resize_(tuple(self.context.get_binding_shape(i)))
            s = self.bindings['images'].shape
            assert im.shape == s, f"input size {im.shape} {'>' if self.dynamic else 'not equal to'} max model size {s}"
            self.binding_addrs['images'] = int(im.data_ptr())
            self.context.execute_v2(list(self.binding_addrs.values()))
            y = [self.bindings[x].data for x in sorted(self.output_names)]
        elif self.coreml:  # CoreML
            im = im.cpu().numpy()
            im = Image.fromarray((im[0] * 255).astype('uint8'))
            # im = im.resize((192, 320), Image.ANTIALIAS)
            y = self.model.predict({'image': im})  # coordinates are xywh normalized
            if 'confidence' in y:
                box = xywh2xyxy(y['coordinates'] * [[w, h, w, h]])  # xyxy pixels
                conf, cls = y['confidence'].max(1), y['confidence'].argmax(1).astype(np.float)
                y = np.concatenate((box, conf.reshape(-1, 1), cls.reshape(-1, 1)), 1)
            else:
                y = list(reversed(y.values()))  # reversed for segmentation models (pred, proto)
        elif self.paddle:  # PaddlePaddle
            im = im.cpu().numpy().astype(np.float32)
            self.input_handle.copy_from_cpu(im)
            self.predictor.run()
            y = [self.predictor.get_output_handle(x).copy_to_cpu() for x in self.output_names]
        elif self.triton:  # NVIDIA Triton Inference Server
            y = self.model(im)
        else:  # TensorFlow (SavedModel, GraphDef, Lite, Edge TPU)
            im = im.cpu().numpy()
            if self.saved_model:  # SavedModel
                y = self.model(im, training=False) if self.keras else self.model(im)
            elif self.pb:  # GraphDef
                y = self.frozen_func(x=self.tf.constant(im))
            else:  # Lite or Edge TPU
                input = self.input_details[0]
                int8 = input['dtype'] == np.uint8  # is TFLite quantized uint8 model
                if int8:
                    scale, zero_point = input['quantization']
                    im = (im / scale + zero_point).astype(np.uint8)  # de-scale
                self.interpreter.set_tensor(input['index'], im)
                self.interpreter.invoke()
                y = []
                for output in self.output_details:
                    x = self.interpreter.get_tensor(output['index'])
                    if int8:
                        scale, zero_point = output['quantization']
                        x = (x.astype(np.float32) - zero_point) * scale  # re-scale
                    y.append(x)
            y = [x if isinstance(x, np.ndarray) else x.numpy() for x in y]
            y[0][..., :4] *= [w, h, w, h]  # xywh normalized to pixels

        if isinstance(y, (list, tuple)):
            return self.from_numpy(y[0]) if len(y) == 1 else [self.from_numpy(x) for x in y]
        else:
            return self.from_numpy(y)

    def from_numpy(self, x):
        return torch.from_numpy(x).to(self.device) if isinstance(x, np.ndarray) else x

    def warmup(self, imgsz=(1, 3, 640, 640)):
        # Warmup model by running inference once
        warmup_types = self.pt, self.jit, self.onnx, self.engine, self.saved_model, self.pb, self.triton
        if any(warmup_types) and (self.device.type != 'cpu' or self.triton):
            im = torch.empty(*imgsz, dtype=torch.half if self.fp16 else torch.float, device=self.device)  # input
            for _ in range(2 if self.jit else 1):  #
                self.forward(im)  # warmup

    @staticmethod
    def _model_type(p='path/to/model.pt'):
        # Return model type from model path, i.e. path='path/to/model.onnx' -> type=onnx
        # types = [pt, jit, onnx, xml, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs, paddle]
        from export import export_formats
        from utils.downloads import is_url
        sf = list(export_formats().Suffix)  # export suffixes
        if not is_url(p, check=False):
            check_suffix(p, sf)  # checks
        url = urlparse(p)  # if url may be Triton inference server
        types = [s in Path(p).name for s in sf]
        types[8] &= not types[9]  # tflite &= not edgetpu
        triton = not any(types) and all([any(s in url.scheme for s in ["http", "grpc"]), url.netloc])
        return types + [triton]

    @staticmethod
    def _load_metadata(f=Path('path/to/meta.yaml')):
        # Load metadata from meta.yaml if it exists
        if f.exists():
            d = yaml_load(f)
            return d['stride'], d['names']  # assign stride, names
        return None, None

'''
# The AutoShape class is a tool that provides flexible input processing and post-processing for YOLOv5 models, enabling object detection across different data sources and environments.
'''
class AutoShape(nn.Module):
    # YOLOv5 input-robust model wrapper for passing cv2/np/PIL/torch inputs. Includes preprocessing, inference and NMS
    '''
    AutoShape class is a tool that provides flexible input processing and post-processing for YOLOv5 models, enabling object detection in different data sources and environments.
    
    conf: Set confidence threshold for NMS. Only detection boxes with confidence above this threshold will be considered as valid detections.
    iou: Set IoU threshold for NMS. Used to determine the degree of overlap between two detection boxes, detection boxes with overlap exceeding this threshold will be suppressed.
    agnostic: When set to True, NMS will ignore the class of detection boxes, i.e., class-agnostic NMS. This means detection boxes of all classes will be treated equally.
    multi_label: Whether to allow multiple labels for each detection box. Usually, only the label with the highest confidence is retained in a detection box.
    classes: Can specify a list of classes to detect. If set, NMS will only retain detection results of specified classes in the list.
    max_det: Maximum detection count limit per image. This helps control the number of output detection boxes, especially when there may be a large number of detection results in the image.
    amp: Indicates whether to enable automatic mixed precision inference. When set to True, half-precision floating point (float16) will be used when possible to accelerate model inference and reduce memory usage.
    '''
    conf = 0.25  # NMS confidence threshold
    iou = 0.45  # NMS IoU threshold
    agnostic = False  # NMS whether to ignore class (class-agnostic)
    multi_label = False  # NMS whether to allow multiple labels per box
    classes = None  # (optional list) filter by class, e.g. [0, 15, 16] represents person, cat, dog in COCO dataset
    max_det = 1000  # maximum detections per image
    amp = False  # whether to enable automatic mixed precision (AMP) inference

    def __init__(self, model, verbose=True):
        super().__init__()  # Call parent class nn.Module constructor
        if verbose:
            LOGGER.info('Adding AutoShape... ')  # If verbose mode is enabled, log information
        copy_attr(self, model, include=('yaml', 'nc', 'hyp', 'names', 'stride', 'abc'), exclude=())  # Copy specific attributes from provided model to AutoShape instance
        self.dmb = isinstance(model, DetectMultiBackend)  # Check if provided model is DetectMultiBackend type
        self.pt = not self.dmb or model.pt  # Determine if model is PyTorch model

        self.model = model.eval()  # Set model to evaluation mode

        if self.pt:
            m = self.model.model.model[-1] if self.dmb else self.model.model[-1]  # Get the last layer of the model
            m.inplace = False  # Set inplace attribute to False, ensuring safety during multi-threaded inference
            m.export = True  # Set export attribute to True, indicating that loss values are not output during inference
    '''
    _apply method ensures that when operations such as .to(device), .cpu(), .cuda(), or .half() are performed on AutoShape instances,
    not only are the model's parameters and cached tensors properly handled, but its internal specific non-parameter attributes are also updated accordingly,
    to ensure consistency and correctness of the model under different devices or data types.
    '''
    def _apply(self, fn):
        # Apply to(), cpu(), cuda(), half() to model tensors that are not parameters or registered buffers
        self = super()._apply(fn)  # Call parent class nn.Module _apply method

        if self.pt:
            # If model is PyTorch model
            m = self.model.model.model[-1] if self.dmb else self.model.model[-1]  # Get the last layer of the model
            m.stride = fn(m.stride)  # Apply function fn to stride attribute
            m.grid = list(map(fn, m.grid))  # Apply function fn to grid attribute

            if isinstance(m.anchor_grid, list):
                m.anchor_grid = list(map(fn, m.anchor_grid))  # If anchor_grid is a list, apply function fn to each element
        return self

    @smart_inference_mode()
    def forward(self, ims, size=640, augment=False, profile=False):
        # Inference from various sources. For size(height=640, width=1280), RGB images example inputs are:
        #   file:        ims = 'data/images/zidane.jpg'  # str or PosixPath
        #   URI:             = 'https://ultralytics.com/images/zidane.jpg'
        #   OpenCV:          = cv2.imread('image.jpg')[:,:,::-1]  # HWC BGR to RGB x(640,1280,3)
        #   PIL:             = Image.open('image.jpg') or ImageGrab.grab()  # HWC x(640,1280,3)
        #   numpy:           = np.zeros((640,1280,3))  # HWC
        #   torch:           = torch.zeros(16,3,320,640)  # BCHW (scaled to size=640, 0-1 values)
        #   multiple:        = [Image.open('image1.jpg'), Image.open('image2.jpg'), ...]  # list of images

        dt = (Profile(), Profile(), Profile())  # Create three performance analysis objects

        with dt[0]:
            # Use the first performance analysis object to monitor the following code block
            if isinstance(size, int):  # Check if a single integer is provided as size
                size = (size, size)  # Expand single size to a tuple, representing width and height

            p = next(self.model.parameters()) if self.pt else torch.empty(1,device=self.model.device)  # Get a model parameter or create an empty tensor
            autocast = self.amp and (p.device.type != 'cpu')  # Determine whether to use automatic mixed precision (AMP) inference based on device type

            if isinstance(ims, torch.Tensor):  # Check if input is a PyTorch tensor
                with amp.autocast(autocast):  # Use automatic mixed precision context manager
                    return self.model(ims.to(p.device).type_as(p), augment=augment)  # Perform model inference in automatic mixed precision environment

            # Pre-process
            n, ims = (len(ims), list(ims)) if isinstance(ims, (list, tuple)) else (1, [ims])  # Convert input to list format, calculate number of images

            shape0, shape1, files = [], [], []  # Initialize lists to store original image dimensions, adjusted dimensions, and filenames

            for i, im in enumerate(ims):
                f = f'image{i}'  # Generate default filename for image

                if isinstance(im, (str, Path)):  # If image is a string or path
                    # If image is a URL, read image via HTTP request, otherwise open file directly
                    im, f = Image.open(requests.get(im, stream=True).raw if str(im).startswith('http') else im), im
                    im = np.asarray(exif_transpose(im))  # Convert image to Numpy array and handle image orientation

                elif isinstance(im, Image.Image):  # If image is a PIL image
                    im, f = np.asarray(exif_transpose(im)), getattr(im, 'filename', f) or f  # Convert image to Numpy array and get filename

                files.append(Path(f).with_suffix('.jpg').name)  # Add processed filename to list

                if im.shape[0] < 5:  # If image is in CHW format (common in PyTorch DataLoader)
                    im = im.transpose((1, 2, 0))  # Convert image to HWC format

                im = im[..., :3] if im.ndim == 3 else cv2.cvtColor(im, cv2.COLOR_GRAY2BGR)  # Ensure image is three-channel RGB

                s = im.shape[:2]  # Get image height and width
                shape0.append(s)  # Add original size to shape0 list

                g = max(size) / max(s)  # Calculate scaling factor
                shape1.append([int(y * g) for y in s])  # Calculate and add adjusted size to shape1 list

                ims[i] = im if im.data.contiguous else np.ascontiguousarray(im)  # Ensure image data is contiguous in memory

            shape1 = [make_divisible(x, self.stride) for x in np.array(shape1).max(0)]  # Adjust shape1 size so it's divisible by model stride
            x = [letterbox(im, shape1, auto=False)[0] for im in ims]  # Apply letterbox function to each image for resizing and padding
            x = np.ascontiguousarray(np.array(x).transpose((0, 3, 1, 2)))  # Convert image array to BCHW format and ensure data is contiguous
            x = torch.from_numpy(x).to(p.device).type_as(p) / 255  # Convert image to PyTorch tensor, move to appropriate device, convert data type, and normalize to 0-1 range

        with amp.autocast(autocast):
            # Use automatic mixed precision (Automatic Mixed Precision, AMP) context manager
            # AMP will use half-precision when possible to accelerate inference and reduce memory usage

            # Inference
            with dt[1]:
                # Use the second performance analysis object to monitor model inference
                y = self.model(x, augment=augment)  # Perform forward inference on preprocessed images

            # Post-process
            with dt[2]:
                # Use the third performance analysis object to monitor post-processing
                # Apply Non-Maximum Suppression (NMS) to filter overlapping detection boxes
                y = non_max_suppression(y if self.dmb else y[0],
                                        self.conf,
                                        self.iou,
                                        self.classes,
                                        self.agnostic,
                                        self.multi_label,
                                        max_det=self.max_det)

                # Adjust detection box sizes to match original image sizes
                for i in range(n):
                    scale_boxes(shape1, y[i][:, :4], shape0[i])

            # Return Detections object containing detection results
            return Detections(ims, y, files, dt, self.names, x.shape)

'''
Detections
    # Class for handling YOLOv5 inference results. Contains methods for managing and displaying object detection results.
'''
class Detections:
    # Class for handling YOLOv5 inference results. The class contains a series of methods for managing and displaying object detection results.
    def __init__(self, ims, pred, files, times=(0, 0, 0), names=None, shape=None):
        # Initialization function
        # ims: image list, pred: prediction result list, files: filename list
        # times: performance analysis time, names: class name list, shape: input shape
        super().__init__()
        d = pred[0].device  # Get device information
        # Calculate normalization factor for coordinate conversion
        gn = [torch.tensor([*(im.shape if i == 0 else im.shape[1:])[::-1], 1, 1], device=d) for im in ims]
        self.ims = ims  # Save image list
        self.pred = pred  # Save prediction result list
        self.names = names  # Save class name list
        self.files = files  # Save filename list
        self.times = times  # Save performance analysis time
        self.xyxy = pred  # Save original prediction result
        self.xywh = [xyxy2xywh(x) for x in pred]  # Convert to xywh format
        self.xyxyn = [x / g for x, g in zip(self.xyxy, gn)]  # Convert to normalized xyxy format
        self.xywhn = [x / g for x, g in zip(self.xywh, gn)]  # Convert to normalized xywh format
        self.n = len(self.pred)  # Save number of images
        self.t = tuple(x.t / self.n * 1E3 for x in times)  # Calculate average processing time
        self.s = tuple(shape)  # Save input shape

    def _run(self, pprint=False, show=False, save=False, crop=False, render=False, labels=True, save_dir=Path('')):
        # Execute method, based on parameters to display, save, crop, or render detection results
        # Parameters include control over whether to print, display, save, crop, render results, and settings for labels and save directory
        s, crops = '', []
        for i, (im, pred) in enumerate(zip(self.ims, self.pred)):
            s += f'\nimage {i + 1}/{len(self.pred)}: {im.shape[0]}x{im.shape[1]} '  # string
            if pred.shape[0]:
                for c in pred[:, -1].unique():
                    n = (pred[:, -1] == c).sum()  # detections per class
                    s += f"{n} {self.names[int(c)]}{'s' * (n > 1)}, "  # add to string
                s = s.rstrip(', ')
                if show or save or render or crop:
                    annotator = Annotator(im, example=str(self.names))
                    for *box, conf, cls in reversed(pred):  # xyxy, confidence, class
                        label = f'{self.names[int(cls)]} {conf:.2f}'
                        if crop:
                            file = save_dir / 'crops' / self.names[int(cls)] / self.files[i] if save else None
                            crops.append({
                                'box': box,
                                'conf': conf,
                                'cls': cls,
                                'label': label,
                                'im': save_one_box(box, im, file=file, save=save)})
                        else:  # all others
                            annotator.box_label(box, label if labels else '', color=colors(cls))
                    im = annotator.im
            else:
                s += '(no detections)'

            im = Image.fromarray(im.astype(np.uint8)) if isinstance(im, np.ndarray) else im  # from np
            if show:
                display(im) if is_notebook() else im.show(self.files[i])
            if save:
                f = self.files[i]
                im.save(save_dir / f)  # save
                if i == self.n - 1:
                    LOGGER.info(f"Saved {self.n} image{'s' * (self.n > 1)} to {colorstr('bold', save_dir)}")
            if render:
                self.ims[i] = np.asarray(im)
        if pprint:
            s = s.lstrip('\n')
            return f'{s}\nSpeed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {self.s}' % self.t
        if crop:
            if save:
                LOGGER.info(f'Saved results to {save_dir}\n')
            return crops

    @TryExcept('Showing images is not supported in this environment')
    def show(self, labels=True):
        self._run(show=True, labels=labels)  # show results

    def save(self, labels=True, save_dir='runs/detect/exp', exist_ok=False):
        save_dir = increment_path(save_dir, exist_ok, mkdir=True)  # increment save_dir
        self._run(save=True, labels=labels, save_dir=save_dir)  # save results

    def crop(self, save=True, save_dir='runs/detect/exp', exist_ok=False):
        save_dir = increment_path(save_dir, exist_ok, mkdir=True) if save else None
        return self._run(crop=True, save=save, save_dir=save_dir)  # crop results

    def render(self, labels=True):
        self._run(render=True, labels=labels)  # render results
        return self.ims

    def pandas(self):
        # return detections as pandas DataFrames, i.e. print(results.pandas().xyxy[0])
        new = copy(self)  # return copy
        ca = 'xmin', 'ymin', 'xmax', 'ymax', 'confidence', 'class', 'name'  # xyxy columns
        cb = 'xcenter', 'ycenter', 'width', 'height', 'confidence', 'class', 'name'  # xywh columns
        for k, c in zip(['xyxy', 'xyxyn', 'xywh', 'xywhn'], [ca, ca, cb, cb]):
            a = [[x[:5] + [int(x[5]), self.names[int(x[5])]] for x in x.tolist()] for x in getattr(self, k)]  # update
            setattr(new, k, [pd.DataFrame(x, columns=c) for x in a])
        return new

    def tolist(self):
        # return a list of Detections objects, i.e. 'for result in results.tolist():'
        r = range(self.n)  # iterable
        x = [Detections([self.ims[i]], [self.pred[i]], [self.files[i]], self.times, self.names, self.s) for i in r]
        # for d in x:
        #    for k in ['ims', 'pred', 'xyxy', 'xyxyn', 'xywh', 'xywhn']:
        #        setattr(d, k, getattr(d, k)[0])  # pop out of list
        return x

    def print(self):
        LOGGER.info(self.__str__())

    def __len__(self):  # override len(results)
        return self.n

    def __str__(self):  # override print(results)
        return self._run(pprint=True)  # print results

    def __repr__(self):
        return f'YOLOv5 {self.__class__} instance\n' + self.__str__()

'''
The Proto class implements the mask prototype (Proto) module in YOLOv5, used for segmentation models.
'''
class Proto(nn.Module):
    # YOLOv5 mask Proto module for segmentation models
    def __init__(self, c1, c_=256, c2=32):  # Constructor: c1=input channels, c_=intermediate channels, c2=output channels
        super().__init__()  # Call nn.Module constructor
        self.cv1 = Conv(c1, c_, k=3)  # First convolutional layer, 3x3 kernel
        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')  # Upsample layer, scale factor 2
        self.cv2 = Conv(c_, c_, k=3)  # Second convolutional layer, 3x3 kernel
        self.cv3 = Conv(c_, c2)  # Third convolutional layer, output layer

    def forward(self, x):
        return self.cv3(self.cv2(self.upsample(self.cv1(x))))

'''
The Classify class is the classification head in the YOLOv5 model, used to convert feature maps to class predictions.
'''
class Classify(nn.Module):
    # YOLOv5 classification head, i.e. x(b,c1,20,20) to x(b,c2)
    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, dropout_p=0.0):
        # Initialization function
        # c1: input channels, c2: output channels, k: kernel size, s: stride, p: padding, g: groups, dropout_p: dropout probability
        super().__init__()  # Call nn.Module constructor
        c_ = 1280  # Intermediate channel number, e.g. feature map size of efficientnet_b0
        self.conv = Conv(c1, c_, k, s, autopad(k, p), g)  # Create a convolutional layer
        self.pool = nn.AdaptiveAvgPool2d(1)  # Adaptive average pooling layer, converts feature map to 1x1
        self.drop = nn.Dropout(p=dropout_p, inplace=True)  # Dropout layer, prevents overfitting
        self.linear = nn.Linear(c_, c2)  # Linear layer for class prediction

    def forward(self, x):
        # Forward function
        if isinstance(x, list):
            x = torch.cat(x, 1)  # If input is a list, concatenate along channel dimension
        return self.linear(self.drop(self.pool(self.conv(x)).flatten(1)))  # Pass through conv, pool, dropout, and linear layers in sequence

import math
from functools import partial
from timm.models._efficientnet_blocks import  SqueezeExcite as SE
from einops import rearrange, reduce

from timm.models.layers import *
from timm.models.layers import DropPath
inplace = True


# SE
class SE(nn.Module):
    def __init__(self, c1, ratio=16):
        super(SE, self).__init__()
        self.avgpool = nn.AdaptiveAvgPool2d(1)
        self.l1 = nn.Linear(c1, c1 // ratio, bias=False)
        self.relu = nn.ReLU(inplace=True)
        self.l2 = nn.Linear(c1 // ratio, c1, bias=False)
        self.sig = nn.Sigmoid()

    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avgpool(x).view(b, c)
        y = self.l1(y)
        y = self.relu(y)
        y = self.l2(y)
        y = self.sig(y)
        y = y.view(b, c, 1, 1)
        return x * y.expand_as(x)


class LayerNorm2d(nn.Module):

    def __init__(self, normalized_shape, eps=1e-6, elementwise_affine=True):
        super().__init__()
        self.norm = nn.LayerNorm(normalized_shape, eps, elementwise_affine)

    def forward(self, x):
        x = rearrange(x, 'b c h w -> b h w c').contiguous()
        x = self.norm(x)
        x = rearrange(x, 'b h w c -> b c h w').contiguous()
        return x


def get_norm(norm_layer='in_1d'):
    eps = 1e-6
    norm_dict = {
        'none': nn.Identity,
        'in_1d': partial(nn.InstanceNorm1d, eps=eps),
        'in_2d': partial(nn.InstanceNorm2d, eps=eps),
        'in_3d': partial(nn.InstanceNorm3d, eps=eps),
        'bn_1d': partial(nn.BatchNorm1d, eps=eps),
        'bn_2d': partial(nn.BatchNorm2d, eps=eps),
        # 'bn_2d': partial(nn.SyncBatchNorm, eps=eps),
        'bn_3d': partial(nn.BatchNorm3d, eps=eps),
        'gn': partial(nn.GroupNorm, eps=eps),
        'ln_1d': partial(nn.LayerNorm, eps=eps),
        'ln_2d': partial(LayerNorm2d, eps=eps),
    }
    return norm_dict[norm_layer]


def get_act(act_layer='relu'):
    act_dict = {
        'none': nn.Identity,
        'sigmoid': Sigmoid,
        'swish': Swish,
        'mish': Mish,
        'hsigmoid': HardSigmoid,
        'hswish': HardSwish,
        'hmish': HardMish,
        'tanh': Tanh,
        'relu': nn.ReLU,
        'relu6': nn.ReLU6,
        'prelu': PReLU,
        'gelu': GELU,
        'silu': nn.SiLU
    }
    return act_dict[act_layer]


class LayerScale(nn.Module):
    def __init__(self, dim, init_values=1e-5, inplace=True):
        super().__init__()
        self.inplace = inplace
        self.gamma = nn.Parameter(init_values * torch.ones(1, 1, dim))

    def forward(self, x):
        return x.mul_(self.gamma) if self.inplace else x * self.gamma


class LayerScale2D(nn.Module):
    def __init__(self, dim, init_values=1e-5, inplace=True):
        super().__init__()
        self.inplace = inplace
        self.gamma = nn.Parameter(init_values * torch.ones(1, dim, 1, 1))

    def forward(self, x):
        return x.mul_(self.gamma) if self.inplace else x * self.gamma


class ConvNormAct(nn.Module):

    def __init__(self, dim_in, dim_out, kernel_size, stride=1, dilation=1, groups=1, bias=False,
                 skip=False, norm_layer='bn_2d', act_layer='relu', inplace=True, drop_path_rate=0.):
        super(ConvNormAct, self).__init__()
        self.has_skip = skip and dim_in == dim_out
        padding = math.ceil((kernel_size - stride) / 2)
        self.conv = nn.Conv2d(dim_in, dim_out, kernel_size, stride, padding, dilation, groups, bias)
        self.norm = get_norm(norm_layer)(dim_out)
        self.act = get_act(act_layer)(inplace=inplace)
        self.drop_path = DropPath(drop_path_rate) if drop_path_rate else nn.Identity()

    def forward(self, x):
        shortcut = x
        x = self.conv(x)
        x = self.norm(x)
        x = self.act(x)
        if self.has_skip:
            x = self.drop_path(x) + shortcut
        return x


# ========== Multi-Scale Populations, for down-sampling and inductive bias ==========
class MSPatchEmb(nn.Module):

    def __init__(self, dim_in, emb_dim, kernel_size=2, c_group=-1, stride=1, dilations=[1, 2, 3],
                 norm_layer='bn_2d', act_layer='silu'):
        super().__init__()
        self.dilation_num = len(dilations)
        assert dim_in % c_group == 0
        c_group = math.gcd(dim_in, emb_dim) if c_group == -1 else c_group
        self.convs = nn.ModuleList()
        for i in range(len(dilations)):
            padding = math.ceil(((kernel_size - 1) * dilations[i] + 1 - stride) / 2)
            self.convs.append(nn.Sequential(
                nn.Conv2d(dim_in, emb_dim, kernel_size, stride, padding, dilations[i], groups=c_group),
                get_norm(norm_layer)(emb_dim),
                get_act(act_layer)(emb_dim)))

    def forward(self, x):
        if self.dilation_num == 1:
            x = self.convs[0](x)
        else:
            x = torch.cat([self.convs[i](x).unsqueeze(dim=-1) for i in range(self.dilation_num)], dim=-1)
            x = reduce(x, 'b c h w n -> b c h w', 'mean').contiguous()
        return x



class iRMB(nn.Module):

    def __init__(self, dim_in, dim_out, norm_in=True, has_skip=True, exp_ratio=1.0, norm_layer='bn_2d',
                 act_layer='relu', v_proj=True, dw_ks=3, stride=1, dilation=1, se_ratio=0.0, dim_head=64, window_size=7,
                 attn_s=True, qkv_bias=False, attn_drop=0., drop=0., drop_path=0., v_group=False, attn_pre=False):
        super().__init__()
        self.norm = get_norm(norm_layer)(dim_in) if norm_in else nn.Identity()
        dim_mid = int(dim_in * exp_ratio)
        self.has_skip = (dim_in == dim_out and stride == 1) and has_skip
        self.attn_s = attn_s
        if self.attn_s:
            assert dim_in % dim_head == 0, 'dim should be divisible by num_heads'
            self.dim_head = dim_head
            self.window_size = window_size
            self.num_head = dim_in // dim_head
            self.scale = self.dim_head ** -0.5
            self.attn_pre = attn_pre
            self.qk = ConvNormAct(dim_in, int(dim_in * 2), kernel_size=1, bias=qkv_bias, norm_layer='none',
                                  act_layer='none')
            self.v = ConvNormAct(dim_in, dim_mid, kernel_size=1, groups=self.num_head if v_group else 1, bias=qkv_bias,
                                 norm_layer='none', act_layer=act_layer, inplace=inplace)
            self.attn_drop = nn.Dropout(attn_drop)
        else:
            if v_proj:
                self.v = ConvNormAct(dim_in, dim_mid, kernel_size=1, bias=qkv_bias, norm_layer='none',
                                     act_layer=act_layer, inplace=inplace)
            else:
                self.v = nn.Identity()
        self.conv_local = ConvNormAct(dim_mid, dim_mid, kernel_size=dw_ks, stride=stride, dilation=dilation,
                                      groups=dim_mid, norm_layer='bn_2d', act_layer='silu', inplace=inplace)
        self.se = SE(dim_mid, rd_ratio=se_ratio, act_layer=get_act(act_layer)) if se_ratio > 0.0 else nn.Identity()

        self.proj_drop = nn.Dropout(drop)
        self.proj = ConvNormAct(dim_mid, dim_out, kernel_size=1, norm_layer='none', act_layer='none', inplace=inplace)
        self.drop_path = DropPath(drop_path) if drop_path else nn.Identity()

    def forward(self, x):
        shortcut = x
        x = self.norm(x)
        B, C, H, W = x.shape
        if self.attn_s:
            # padding
            if self.window_size <= 0:
                window_size_W, window_size_H = W, H
            else:
                window_size_W, window_size_H = self.window_size, self.window_size
            pad_l, pad_t = 0, 0
            pad_r = (window_size_W - W % window_size_W) % window_size_W
            pad_b = (window_size_H - H % window_size_H) % window_size_H
            x = F.pad(x, (pad_l, pad_r, pad_t, pad_b, 0, 0,))
            n1, n2 = (H + pad_b) // window_size_H, (W + pad_r) // window_size_W
            x = rearrange(x, 'b c (h1 n1) (w1 n2) -> (b n1 n2) c h1 w1', n1=n1, n2=n2).contiguous()
            # attention
            b, c, h, w = x.shape
            qk = self.qk(x)
            qk = rearrange(qk, 'b (qk heads dim_head) h w -> qk b heads (h w) dim_head', qk=2, heads=self.num_head,
                           dim_head=self.dim_head).contiguous()
            q, k = qk[0], qk[1]
            attn_spa = (q @ k.transpose(-2, -1)) * self.scale
            attn_spa = attn_spa.softmax(dim=-1)
            attn_spa = self.attn_drop(attn_spa)
            if self.attn_pre:
                x = rearrange(x, 'b (heads dim_head) h w -> b heads (h w) dim_head', heads=self.num_head).contiguous()
                x_spa = attn_spa @ x
                x_spa = rearrange(x_spa, 'b heads (h w) dim_head -> b (heads dim_head) h w', heads=self.num_head, h=h,
                                  w=w).contiguous()
                x_spa = self.v(x_spa)
            else:
                v = self.v(x)
                v = rearrange(v, 'b (heads dim_head) h w -> b heads (h w) dim_head', heads=self.num_head).contiguous()
                x_spa = attn_spa @ v
                x_spa = rearrange(x_spa, 'b heads (h w) dim_head -> b (heads dim_head) h w', heads=self.num_head, h=h,
                                  w=w).contiguous()
            # unpadding
            x = rearrange(x_spa, '(b n1 n2) c h1 w1 -> b c (h1 n1) (w1 n2)', n1=n1, n2=n2).contiguous()
            if pad_r > 0 or pad_b > 0:
                x = x[:, :, :H, :W].contiguous()
        else:
            x = self.v(x)

        x = x + self.se(self.conv_local(x)) if self.has_skip else self.se(self.conv_local(x))

        x = self.proj_drop(x)
        x = self.proj(x)

        x = (shortcut + self.drop_path(x)) if self.has_skip else x
        return x

"""
https://arxiv.org/abs/2303.03667
<<Run, Don't Walk: Chasing Higher FLOPS for Faster Neural Networks>>
"""
# --------------------------FasterNet----------------------------
from timm.models.layers import DropPath


class Partial_conv3(nn.Module):
    def __init__(self, dim, n_div, forward):
        super().__init__()  # Call parent class initializer
        self.dim_conv3 = dim // n_div  # Dimension for convolution operation
        self.dim_untouched = dim - self.dim_conv3  # Dimension left untouched
        self.partial_conv3 = nn.Conv2d(self.dim_conv3, self.dim_conv3, 3, 1, 1, bias=False)  # Define partial convolution layer

        if forward == 'slicing':  # If forward method is 'slicing'
            self.forward = self.forward_slicing  # Set forward method to forward_slicing
        elif forward == 'split_cat':  # If forward method is 'split_cat'
            self.forward = self.forward_split_cat  # Set forward method to forward_split_cat
        else:  # If forward method is neither 'slicing' nor 'split_cat'
            raise NotImplementedError  # Raise not implemented error

    def forward_slicing(self, x):
        # Only used for inference
        x = x.clone()  # Clone input x to keep original input unchanged, for later residual connection
        x[:, :self.dim_conv3, :, :] = self.partial_conv3(x[:, :self.dim_conv3, :, :])  # Apply partial convolution to part of the input

        return x  # Return processed x

    def forward_split_cat(self, x):
        # Used for training/inference
        x1, x2 = torch.split(x, [self.dim_conv3, self.dim_untouched], dim=1)  # Split input x into two parts
        x1 = self.partial_conv3(x1)  # Apply partial convolution to x1 part
        x = torch.cat((x1, x2), 1)  # Concatenate convolved x1 and untouched x2
        return x  # Return processed x



class MLPBlock(nn.Module):
    def __init__(self,
                 dim,
                 n_div,
                 mlp_ratio,
                 drop_path,
                 layer_scale_init_value,
                 act_layer,
                 norm_layer,
                 pconv_fw_type
                 ):

        super().__init__()  # Call parent class initializer
        self.dim = dim  # Input dimension
        self.mlp_ratio = mlp_ratio  # Hidden layer dimension multiplier for MLP block
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()  # DropPath or identity mapping
        self.n_div = n_div  # Split parameter for partial convolution

        mlp_hidden_dim = int(dim * mlp_ratio)  # Calculate hidden layer dimension for MLP
        mlp_layer = [
            nn.Conv2d(dim, mlp_hidden_dim, 1, bias=False),  # 1x1 convolution, expand dimension
            norm_layer(mlp_hidden_dim),  # Normalization layer
            act_layer(),  # Activation layer
            nn.Conv2d(mlp_hidden_dim, dim, 1, bias=False)  # 1x1 convolution, restore dimension
        ]
        self.mlp = nn.Sequential(*mlp_layer)  # Sequentialize MLP layers
        self.spatial_mixing = Partial_conv3(
            dim,
            n_div,
            pconv_fw_type
        )  # Partial convolution for spatial mixing
        if layer_scale_init_value > 0:
            self.layer_scale = nn.Parameter(layer_scale_init_value * torch.ones((dim)), requires_grad=True)  # Layer scale parameter
            self.forward = self.forward_layer_scale  # Use forward with layer scale if layer_scale_init_value > 0
        else:
            self.forward = self.forward  # Otherwise use standard forward

    def forward(self, x):
        shortcut = x  # Residual connection
        x = self.spatial_mixing(x)  # Apply spatial mixing
        x = shortcut + self.drop_path(self.mlp(x))  # Apply MLP and add residual
        return x  # Return result

    def forward_layer_scale(self, x):
        shortcut = x  # Residual connection
        x = self.spatial_mixing(x)  # Apply spatial mixing
        x = shortcut + self.drop_path(
            self.layer_scale.unsqueeze(-1).unsqueeze(-1) * self.mlp(x))  # Apply MLP with layer scale and add residual
        return x  # Return result


class BasicStage(nn.Module):
    def __init__(self,
                 dim,
                 depth=1,
                 n_div=4,
                 mlp_ratio=2,
                 layer_scale_init_value=0,
                 norm_layer=nn.BatchNorm2d,
                 act_layer=nn.ReLU,
                 pconv_fw_type='split_cat'
                 ):
        super().__init__()
        dpr = [x.item()
               for x in torch.linspace(0, 0.0, sum([1, 2, 8, 2]))]
        blocks_list = [
            MLPBlock(
                dim=dim,
                n_div=n_div,
                mlp_ratio=mlp_ratio,
                drop_path=dpr[i],
                layer_scale_init_value=layer_scale_init_value,
                norm_layer=norm_layer,
                act_layer=act_layer,
                pconv_fw_type=pconv_fw_type
            )
            for i in range(depth)
        ]

        self.blocks = nn.Sequential(*blocks_list)

    def forward(self, x):
        x = self.blocks(x)
        return x


class PatchEmbed_FasterNet(nn.Module):
    def __init__(self, in_chans, embed_dim, patch_size, patch_stride, norm_layer=nn.BatchNorm2d):
        super().__init__()  # Call parent class initializer
        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_stride, bias=False)  # Define a convolutional layer to split the input image into multiple embeddings
        if norm_layer is not None:  # If a normalization layer is specified
            self.norm = norm_layer(embed_dim)  # Create a normalization layer
        else:  # If no normalization layer is specified
            self.norm = nn.Identity()  # Use identity mapping, i.e., do nothing

    def forward(self, x):
        x = self.norm(self.proj(x))  # First use the convolutional layer for embedding, then apply normalization to the embedding result
        return x  # Return the processed result

    def fuseforward(self, x):
        x = self.proj(x)  # Directly use the convolutional layer for embedding, without applying normalization
        return x  # Return the processed result


class PatchMerging_FasterNet(nn.Module):
    def __init__(self, dim, out_dim, k, patch_stride2, norm_layer=nn.BatchNorm2d):
        super().__init__()  # Call parent class initializer
        self.reduction = nn.Conv2d(dim, out_dim, kernel_size=k, stride=patch_stride2, bias=False)  # Define a convolutional layer for dimensionality reduction and merging adjacent embeddings
        if norm_layer is not None:  # If a normalization layer is specified
            self.norm = norm_layer(out_dim)  # Create a normalization layer
        else:  # If no normalization layer is specified
            self.norm = nn.Identity()  # Use identity mapping, i.e., do nothing

    def forward(self, x):
        x = self.norm(self.reduction(x))  # First use the convolutional layer for dimensionality reduction and merging, then apply normalization to the result
        return x  # Return the processed result

    def fuseforward(self, x):
        x = self.reduction(x)  # Directly use the convolutional layer for dimensionality reduction and merging, without applying normalization
        return x  # Return the processed result

# -----------------------------------
class h_sigmoid(nn.Module):
    def __init__(self, inplace=True):
        super(h_sigmoid, self).__init__()
        self.relu = nn.ReLU6(inplace=inplace)

    def forward(self, x):
        return self.relu(x + 3) / 6


class h_swish(nn.Module):
    def __init__(self, inplace=True):
        super(h_swish, self).__init__()
        self.sigmoid = h_sigmoid(inplace=inplace)

    def forward(self, x):
        return x * self.sigmoid(x)


class CoordAtt(nn.Module):
    def __init__(self, inp, oup, reduction=32):
        super(CoordAtt, self).__init__()
        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))
        self.pool_w = nn.AdaptiveAvgPool2d((1, None))
        mip = max(8, inp // reduction)
        self.conv1 = nn.Conv2d(inp, mip, kernel_size=1, stride=1, padding=0)
        self.bn1 = nn.BatchNorm2d(mip)
        self.act = h_swish()
        self.conv_h = nn.Conv2d(mip, oup, kernel_size=1, stride=1, padding=0)
        self.conv_w = nn.Conv2d(mip, oup, kernel_size=1, stride=1, padding=0)

    def forward(self, x):
        identity = x
        n, c, h, w = x.size()
        x_h = self.pool_h(x)
        x_w = self.pool_w(x).permute(0, 1, 3, 2)
        y = torch.cat([x_h, x_w], dim=2)
        y = self.conv1(y)
        y = self.bn1(y)
        y = self.act(y)
        x_h, x_w = torch.split(y, [h, w], dim=2)
        x_w = x_w.permute(0, 1, 3, 2)
        a_h = self.conv_h(x_h).sigmoid()
        a_w = self.conv_w(x_w).sigmoid()
        out = identity * a_w * a_h
        return out


class CA_Bottleneck(nn.Module):
    #  Bottleneck with 1 Attention
    def __init__(self, c1, c2, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, shortcut, groups, expansion
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = Conv(c_, c2, 3, 1, g=g)
        self.ca = CoordAtt(c2, c2, 32)
        self.add = shortcut and c1 == c2

    def forward(self, x):
        return x + self.ca(self.cv2(self.cv1(x))) if self.add else self.ca(self.cv2(self.cv1(x)))


class C3_CA(nn.Module):
    # CSP Bottleneck with 3 convolutions and 1 CA.
    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = Conv(c1, c_, 1, 1)
        self.cv3 = Conv(2 * c_, c2, 1)
        self.m = nn.Sequential(*(CA_Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)))

    def forward(self, x):
        return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), dim=1))

# ---------------------------CA End---------------------------
# BiFPN
# Add operation for two feature maps
class BiFPN_Add2(nn.Module):
    def __init__(self, c1, c2):
        super(BiFPN_Add2, self).__init__()
        # Set learnable parameters. The purpose of nn.Parameter is to convert a non-trainable Tensor into a trainable parameter,
        # and register it as part of the host model. That is, model.parameters() will include this parameter,
        # so it will be automatically optimized during parameter optimization.
        self.w = nn.Parameter(torch.ones(2, dtype=torch.float32), requires_grad=True)
        self.epsilon = 0.0001
        self.conv = nn.Conv2d(c1, c2, kernel_size=1, stride=1, padding=0)
        self.silu = nn.SiLU()

    def forward(self, x):
        w = self.w
        weight = w / (torch.sum(w, dim=0) + self.epsilon)
        return self.conv(self.silu(weight[0] * x[0] + weight[1] * x[1]))


# Add operation for three feature maps
class BiFPN_Add3(nn.Module):
    def __init__(self, c1, c2):
        super(BiFPN_Add3, self).__init__()
        self.w = nn.Parameter(torch.ones(3, dtype=torch.float32), requires_grad=True)
        self.epsilon = 0.0001
        self.conv = nn.Conv2d(c1, c2, kernel_size=1, stride=1, padding=0)
        self.silu = nn.SiLU()

    def forward(self, x):
        w = self.w
        weight = w / (torch.sum(w, dim=0) + self.epsilon)
        # Fast normalized fusion
        return self.conv(self.silu(weight[0] * x[0] + weight[1] * x[1] + weight[2] * x[2]))

import torch
from torch import nn


class SeBlock(nn.Module):
    def __init__(self, in_channel, reduction=4):
        super().__init__()
        self.Squeeze = nn.AdaptiveAvgPool2d(1)

        self.Excitation = nn.Sequential()
        self.Excitation.add_module(
            "FC1", nn.Conv2d(in_channel, in_channel // reduction, kernel_size=1)
        )
        self.Excitation.add_module("ReLU", nn.ReLU())
        self.Excitation.add_module(
            "FC2", nn.Conv2d(in_channel // reduction, in_channel, kernel_size=1)
        )
        self.Excitation.add_module("Sigmoid", nn.Sigmoid())

    def forward(self, x):
        y = self.Squeeze(x)
        ouput = self.Excitation(y)
        return x * (ouput.expand_as(x))


class Conv_BN_HSwish(nn.Module):
    def __init__(self, c1, c2, stride):
        super(Conv_BN_HSwish, self).__init__()
        self.conv = nn.Conv2d(c1, c2, 3, stride, 1, bias=False)
        self.bn = nn.BatchNorm2d(c2)
        self.act = nn.Hardswish()

    def forward(self, x):
        return self.act(self.bn(self.conv(x)))


class MobileNetV3_InvertedResidual(nn.Module):
    def __init__(self, inp, oup, hidden_dim, kernel_size, stride, use_se, use_hs):
        super(MobileNetV3_InvertedResidual, self).__init__()
        assert stride in [1, 2]

        self.identity = stride == 1 and inp == oup

        if inp == hidden_dim:
            self.conv = nn.Sequential(
                # dw
                nn.Conv2d(
                    hidden_dim,
                    hidden_dim,
                    kernel_size,
                    stride,
                    (kernel_size - 1) // 2,
                    groups=hidden_dim,
                    bias=False,
                ),
                nn.BatchNorm2d(hidden_dim),
                nn.Hardswish() if use_hs else nn.ReLU(),
                # Squeeze-and-Excite
                SeBlock(hidden_dim) if use_se else nn.Sequential(),
                # pw-linear
                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),
                nn.BatchNorm2d(oup),
            )
        else:
            self.conv = nn.Sequential(
                # pw
                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),
                nn.BatchNorm2d(hidden_dim),
                nn.Hardswish() if use_hs else nn.ReLU(),
                # dw
                nn.Conv2d(
                    hidden_dim,
                    hidden_dim,
                    kernel_size,
                    stride,
                    (kernel_size - 1) // 2,
                    groups=hidden_dim,
                    bias=False,
                ),
                nn.BatchNorm2d(hidden_dim),
                # Squeeze-and-Excite
                SeBlock(hidden_dim) if use_se else nn.Sequential(),
                nn.Hardswish() if use_hs else nn.ReLU(),
                # pw-linear
                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),
                nn.BatchNorm2d(oup),
            )

    def forward(self, x):
        y = self.conv(x)
        if self.identity:
            return x + y
        else:
            return y


import torch
from torch import nn


def channel_shuffle(x, groups):
    batchsize, num_channels, height, width = x.data.size()
    channels_per_group = num_channels // groups
    x = x.view(batchsize, groups, channels_per_group, height, width)
    x = torch.transpose(x, 1, 2).contiguous()
    x = x.view(batchsize, -1, height, width)

    return x


class CBRM(nn.Module):  # conv BN ReLU Maxpool2d
    def __init__(self, c1, c2):
        super(CBRM, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(c1, c2, kernel_size=3, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(c2),
            nn.ReLU(inplace=True),
        )
        self.maxpool = nn.MaxPool2d(
            kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False
        )

    def forward(self, x):
        return self.maxpool(self.conv(x))


class Shuffle_Block(nn.Module):
    def __init__(self, ch_in, ch_out, stride):
        super(Shuffle_Block, self).__init__()

        if not (1 <= stride <= 2):
            raise ValueError("illegal stride value")
        self.stride = stride

        branch_features = ch_out // 2
        assert (self.stride != 1) or (ch_in == branch_features << 1)

        if self.stride > 1:
            self.branch1 = nn.Sequential(
                self.depthwise_conv(
                    ch_in, ch_in, kernel_size=3, stride=self.stride, padding=1
                ),
                nn.BatchNorm2d(ch_in),
                nn.Conv2d(
                    ch_in,
                    branch_features,
                    kernel_size=1,
                    stride=1,
                    padding=0,
                    bias=False,
                ),
                nn.BatchNorm2d(branch_features),
                nn.ReLU(inplace=True),
            )

        self.branch2 = nn.Sequential(
            nn.Conv2d(
                ch_in if (self.stride > 1) else branch_features,
                branch_features,
                kernel_size=1,
                stride=1,
                padding=0,
                bias=False,
            ),
            nn.BatchNorm2d(branch_features),
            nn.ReLU(inplace=True),
            self.depthwise_conv(
                branch_features,
                branch_features,
                kernel_size=3,
                stride=self.stride,
                padding=1,
            ),
            nn.BatchNorm2d(branch_features),
            nn.Conv2d(
                branch_features,
                branch_features,
                kernel_size=1,
                stride=1,
                padding=0,
                bias=False,
            ),
            nn.BatchNorm2d(branch_features),
            nn.ReLU(inplace=True),
        )

    @staticmethod
    def depthwise_conv(i, o, kernel_size, stride=1, padding=0, bias=False):
        return nn.Conv2d(i, o, kernel_size, stride, padding, bias=bias, groups=i)

    def forward(self, x):
        if self.stride == 1:
            x1, x2 = x.chunk(2, dim=1)
            out = torch.cat((x1, self.branch2(x2)), dim=1)
        else:
            out = torch.cat((self.branch1(x), self.branch2(x)), dim=1)

        out = channel_shuffle(out, 2)

        return out

import torch
from torch import nn


def autopad(k, p=None, d=1):  # kernel, padding, dilation
    # Pad to 'same' shape outputs
    if d > 1:
        k = (
            d * (k - 1) + 1 if isinstance(k, int) else [d * (x - 1) + 1 for x in k]
        )  # actual kernel-size
    if p is None:
        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad
    return p


class Conv(nn.Module):
    # Standard convolution with args(ch_in, ch_out, kernel, stride, padding, groups, dilation, activation)
    default_act = nn.SiLU()  # default activation

    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):
        super().__init__()
        self.conv = nn.Conv2d(
            c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False
        )
        self.bn = nn.BatchNorm2d(c2)
        self.act = (
            self.default_act
            if act is True
            else act if isinstance(act, nn.Module) else nn.Identity()
        )

    def forward(self, x):
        return self.act(self.bn(self.conv(x)))

    def forward_fuse(self, x):
        return self.act(self.conv(x))


class GhostConv(nn.Module):
    # Ghost Convolution https://github.com/huawei-noah/ghostnet
    def __init__(
        self, c1, c2, k=1, s=1, g=1, act=True
    ):  # ch_in, ch_out, kernel, stride, groups
        super().__init__()
        c_ = c2 // 2  # hidden channels
        self.cv1 = Conv(c1, c_, k, s, None, g, act=act)
        self.cv2 = Conv(c_, c_, 5, 1, None, c_, act=act)

    def forward(self, x):
        y = self.cv1(x)
        return torch.cat((y, self.cv2(y)), 1)


class SeBlock(nn.Module):
    def __init__(self, in_channel, reduction=4):
        super().__init__()
        self.Squeeze = nn.AdaptiveAvgPool2d(1)

        self.Excitation = nn.Sequential()
        self.Excitation.add_module(
            "FC1", nn.Conv2d(in_channel, in_channel // reduction, kernel_size=1)
        )
        self.Excitation.add_module("ReLU", nn.ReLU())
        self.Excitation.add_module(
            "FC2", nn.Conv2d(in_channel // reduction, in_channel, kernel_size=1)
        )
        self.Excitation.add_module("Sigmoid", nn.Sigmoid())

    def forward(self, x):
        y = self.Squeeze(x)
        ouput = self.Excitation(y)
        return x * (ouput.expand_as(x))


class G_bneck(nn.Module):
    # Ghost Bottleneck https://github.com/huawei-noah/ghostnet
    def __init__(
        self, c1, c2, midc, k=5, s=1, use_se=False
    ):  # ch_in, ch_mid, ch_out, kernel, stride, use_se
        super().__init__()
        assert s in [1, 2]
        c_ = midc
        self.conv = nn.Sequential(
            GhostConv(c1, c_, 1, 1),  # Expansion
            (
                Conv(c_, c_, 3, s=2, p=1, g=c_, act=False) if s == 2 else nn.Identity()
            ),  # dw
            # Squeeze-and-Excite
            SeBlock(c_) if use_se else nn.Sequential(),
            GhostConv(c_, c2, 1, 1, act=False),
        )  # Squeeze pw-linear

        self.shortcut = (
            nn.Identity()
            if (c1 == c2 and s == 1)
            else nn.Sequential(
                Conv(c1, c1, 3, s=s, p=1, g=c1, act=False),
                Conv(c1, c2, 1, 1, act=False),
            )
        )

    def forward(self, x):
        # print(self.conv(x).shape)
        # print(self.shortcut(x).shape)
        return self.conv(x) + self.shortcut(x)

import torch
from torch import nn


class SeBlock(nn.Module):
    def __init__(self, in_channel, reduction=4):
        super().__init__()
        self.Squeeze = nn.AdaptiveAvgPool2d(1)

        self.Excitation = nn.Sequential()
        self.Excitation.add_module(
            "FC1", nn.Conv2d(in_channel, in_channel // reduction, kernel_size=1)
        )
        self.Excitation.add_module("ReLU", nn.ReLU())
        self.Excitation.add_module(
            "FC2", nn.Conv2d(in_channel // reduction, in_channel, kernel_size=1)
        )
        self.Excitation.add_module("Sigmoid", nn.Sigmoid())

    def forward(self, x):
        y = self.Squeeze(x)
        ouput = self.Excitation(y)
        return x * (ouput.expand_as(x))


class drop_connect:
    def __init__(self, drop_connect_rate):
        self.drop_connect_rate = drop_connect_rate

    def forward(self, x, training):
        if not training:
            return x
        keep_prob = 1.0 - self.drop_connect_rate
        batch_size = x.shape[0]
        random_tensor = keep_prob
        random_tensor += torch.rand(
            [batch_size, 1, 1, 1], dtype=x.dtype, device=x.device
        )
        binary_mask = torch.floor(random_tensor)  # 1
        x = (x / keep_prob) * binary_mask
        return x


class stem(nn.Module):
    def __init__(self, c1, c2, act="ReLU6"):
        super().__init__()
        self.conv = nn.Conv2d(c1, c2, kernel_size=3, stride=2, padding=1, bias=False)
        self.bn = nn.BatchNorm2d(num_features=c2)
        if act == "ReLU6":
            self.act = nn.ReLU6(inplace=True)

    def forward(self, x):
        return self.act(self.bn(self.conv(x)))


class MBConvBlock(nn.Module):
    def __init__(
        self, inp, final_oup, k, s, expand_ratio, drop_connect_rate, has_se=False
    ):
        super(MBConvBlock, self).__init__()

        self._momentum = 0.01
        self._epsilon = 1e-3
        self.input_filters = inp
        self.output_filters = final_oup
        self.stride = s
        self.expand_ratio = expand_ratio
        self.has_se = has_se
        self.id_skip = True  # skip connection and drop connect
        se_ratio = 0.25

        # Expansion phase
        oup = inp * expand_ratio  # number of output channels
        if expand_ratio != 1:
            self._expand_conv = nn.Conv2d(
                in_channels=inp, out_channels=oup, kernel_size=1, bias=False
            )
            self._bn0 = nn.BatchNorm2d(
                num_features=oup, momentum=self._momentum, eps=self._epsilon
            )

        # Depthwise convolution phase
        self._depthwise_conv = nn.Conv2d(
            in_channels=oup,
            out_channels=oup,
            groups=oup,  # groups makes it depthwise
            kernel_size=k,
            padding=(k - 1) // 2,
            stride=s,
            bias=False,
        )
        self._bn1 = nn.BatchNorm2d(
            num_features=oup, momentum=self._momentum, eps=self._epsilon
        )

        # Squeeze and Excitation layer, if desired
        if self.has_se:
            num_squeezed_channels = max(1, int(inp * se_ratio))
            self.se = SeBlock(oup, 4)

        # Output phase
        self._project_conv = nn.Conv2d(
            in_channels=oup, out_channels=final_oup, kernel_size=1, bias=False
        )
        self._bn2 = nn.BatchNorm2d(
            num_features=final_oup, momentum=self._momentum, eps=self._epsilon
        )
        self._relu = nn.ReLU6(inplace=True)

        self.drop_connect = drop_connect(drop_connect_rate)

    def forward(self, x, drop_connect_rate=None):
        """
        :param x: input tensor
        :param drop_connect_rate: drop connect rate (float, between 0 and 1)
        :return: output of block
        """

        # Expansion and Depthwise Convolution
        identity = x
        if self.expand_ratio != 1:
            x = self._relu(self._bn0(self._expand_conv(x)))
        x = self._relu(self._bn1(self._depthwise_conv(x)))

        # Squeeze and Excitation
        if self.has_se:
            x = self.se(x)

        x = self._bn2(self._project_conv(x))

        # Skip connection and drop connect
        if (
            self.id_skip
            and self.stride == 1
            and self.input_filters == self.output_filters
        ):
            if drop_connect_rate:
                x = self.drop_connect(x, training=self.training)
            x += identity  # skip connection
        return x




