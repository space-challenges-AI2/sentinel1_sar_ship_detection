{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e6fd8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.15.0 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (2.15.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: keras==2.15.0 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: protobuf==4.25.3 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (4.25.3)\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from tensorflow==2.15.0) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (3.14.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (25.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (78.1.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (4.12.2)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (1.74.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (2.15.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (2.40.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (1.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (3.8.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (2.32.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (3.1.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (4.9.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (2.0.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (2025.8.3)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (0.6.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow==2.15.0) (0.45.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (3.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (3.0.2)\n",
      "Requirement already satisfied: yolov5 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (7.0.14)\n",
      "Requirement already satisfied: pandas in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: torch in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (0.22.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (3.10.5)\n",
      "Requirement already satisfied: Pillow in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (11.3.0)\n",
      "Requirement already satisfied: gitpython>=3.1.30 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from yolov5) (3.1.45)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from yolov5) (1.26.4)\n",
      "Requirement already satisfied: psutil in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from yolov5) (5.9.0)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from yolov5) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from yolov5) (2.32.4)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from yolov5) (1.15.3)\n",
      "Requirement already satisfied: thop>=0.1.1 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from yolov5) (0.1.1.post2209072238)\n",
      "Requirement already satisfied: ultralytics>=8.0.100 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from yolov5) (8.3.174)\n",
      "Requirement already satisfied: tensorboard>=2.4.1 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from yolov5) (2.15.2)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from yolov5) (0.13.2)\n",
      "Requirement already satisfied: setuptools>=65.5.1 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from yolov5) (78.1.1)\n",
      "Requirement already satisfied: fire in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from yolov5) (0.7.0)\n",
      "Requirement already satisfied: boto3>=1.19.1 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from yolov5) (1.40.3)\n",
      "Requirement already satisfied: sahi>=0.11.10 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from yolov5) (0.11.32)\n",
      "Requirement already satisfied: huggingface-hub<0.25.0,>=0.12.0 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from yolov5) (0.24.7)\n",
      "Requirement already satisfied: roboflow>=0.2.29 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from yolov5) (1.2.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from huggingface-hub<0.25.0,>=0.12.0->yolov5) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from huggingface-hub<0.25.0,>=0.12.0->yolov5) (2025.7.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from huggingface-hub<0.25.0,>=0.12.0->yolov5) (25.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from huggingface-hub<0.25.0,>=0.12.0->yolov5) (4.12.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from matplotlib) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: botocore<1.41.0,>=1.40.3 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from boto3>=1.19.1->yolov5) (1.40.3)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from boto3>=1.19.1->yolov5) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from boto3>=1.19.1->yolov5) (0.13.1)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from botocore<1.41.0,>=1.40.3->boto3>=1.19.1->yolov5) (2.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from gitpython>=3.1.30->yolov5) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.30->yolov5) (5.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from requests>=2.23.0->yolov5) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from requests>=2.23.0->yolov5) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from requests>=2.23.0->yolov5) (2025.8.3)\n",
      "Requirement already satisfied: opencv-python-headless==4.10.0.84 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from roboflow>=0.2.29->yolov5) (4.10.0.84)\n",
      "Requirement already satisfied: pi-heif<2 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from roboflow>=0.2.29->yolov5) (1.1.0)\n",
      "Requirement already satisfied: pillow-avif-plugin<2 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from roboflow>=0.2.29->yolov5) (1.5.2)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from roboflow>=0.2.29->yolov5) (1.1.1)\n",
      "Requirement already satisfied: requests-toolbelt in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from roboflow>=0.2.29->yolov5) (1.0.0)\n",
      "Requirement already satisfied: filetype in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from roboflow>=0.2.29->yolov5) (1.2.0)\n",
      "Requirement already satisfied: click in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from sahi>=0.11.10->yolov5) (8.2.1)\n",
      "Requirement already satisfied: pybboxes==0.1.6 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from sahi>=0.11.10->yolov5) (0.1.6)\n",
      "Requirement already satisfied: shapely>=2.0.0 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from sahi>=0.11.10->yolov5) (2.1.1)\n",
      "Requirement already satisfied: terminaltables in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from sahi>=0.11.10->yolov5) (3.1.10)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from tensorboard>=2.4.1->yolov5) (2.3.1)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from tensorboard>=2.4.1->yolov5) (1.74.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from tensorboard>=2.4.1->yolov5) (2.40.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from tensorboard>=2.4.1->yolov5) (1.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from tensorboard>=2.4.1->yolov5) (3.8.2)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from tensorboard>=2.4.1->yolov5) (4.25.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from tensorboard>=2.4.1->yolov5) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from tensorboard>=2.4.1->yolov5) (3.1.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->yolov5) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->yolov5) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->yolov5) (4.9.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.4.1->yolov5) (2.0.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth<3,>=1.6.3->tensorboard>=2.4.1->yolov5) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.4.1->yolov5) (3.3.1)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from ultralytics>=8.0.100->yolov5) (9.0.0)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from ultralytics>=8.0.100->yolov5) (2.0.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard>=2.4.1->yolov5) (3.0.2)\n",
      "Requirement already satisfied: termcolor in c:\\users\\angel\\miniconda3\\envs\\tf_env\\lib\\site-packages (from fire->yolov5) (3.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Setup complete. All libraries are installed and imported.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Imports\n",
    "\n",
    "# --- Package Installation ---\n",
    "# Install necessary packages with specific versions for compatibility\n",
    "%pip install tensorflow==2.15.0 keras==2.15.0 protobuf==4.25.3\n",
    "%pip install yolov5 pandas opencv-python torch torchvision tqdm matplotlib Pillow\n",
    "\n",
    "# --- Library Imports ---\n",
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# --- Environment Configuration ---\n",
    "# Suppress verbose logs for a cleaner output\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Setup complete. All libraries are installed and imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0774869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLOv5 model from 'best_yolo_completeDataset.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  v7.0-422-g2540fd4c Python-3.10.18 torch-2.7.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5n summary: 157 layers, 1760518 parameters, 0 gradients, 4.1 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ YOLOv5 model loaded successfully.\n",
      "Loading classifier model from 'ship_classifier.h5'...\n",
      "✅ Classifier model loaded successfully. Classes: ['Ship', 'No ship']\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Model Loading\n",
    "\n",
    "# --- Configuration ---\n",
    "YOLO_MODEL_PATH = 'best_yolo_completeDataset.pt'\n",
    "CLASSIFIER_MODEL_PATH = 'ship_classifier.h5'\n",
    "CLASSIFIER_LABELS_PATH = 'labels.txt'\n",
    "YOLO_REPO_PATH = 'yolov5' # Assumes yolov5 repo is cloned\n",
    "\n",
    "# --- Load YOLOv5 Detection Model ---\n",
    "print(f\"Loading YOLOv5 model from '{YOLO_MODEL_PATH}'...\")\n",
    "try:\n",
    "    yolo_model = torch.hub.load(YOLO_REPO_PATH, 'custom', path=YOLO_MODEL_PATH, source='local', force_reload=True)\n",
    "    if torch.cuda.is_available():\n",
    "        yolo_model.to('cuda')\n",
    "    print(\"✅ YOLOv5 model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading YOLOv5 model: {e}\")\n",
    "\n",
    "# --- Load Keras/TF Classification Model ---\n",
    "print(f\"Loading classifier model from '{CLASSIFIER_MODEL_PATH}'...\")\n",
    "try:\n",
    "    classifier_model = tf.keras.models.load_model(CLASSIFIER_MODEL_PATH, compile=False)\n",
    "    with open(CLASSIFIER_LABELS_PATH, 'r') as f:\n",
    "        classifier_labels = [line.strip().split(' ', 1)[1] for line in f]\n",
    "    print(f\"✅ Classifier model loaded successfully. Classes: {classifier_labels}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading classifier model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "277b9130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Helper Functions\n",
    "\n",
    "def calculate_iou(boxA, boxB):\n",
    "    \"\"\"Calculates the Intersection over Union (IoU) between two bounding boxes.\"\"\"\n",
    "    interArea = max(0, min(boxA[2], boxB[2]) - max(boxA[0], boxA[0])) * max(0, min(boxA[3], boxB[3]) - max(boxA[1], boxA[1]))\n",
    "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "    unionArea = float(boxAArea + boxBArea - interArea)\n",
    "    return interArea / unionArea if unionArea > 0 else 0\n",
    "\n",
    "def load_yolo_labels(label_path, img_width, img_height):\n",
    "    \"\"\"Loads ground truth boxes from a YOLO .txt file and denormalizes them.\"\"\"\n",
    "    boxes = []\n",
    "    if not os.path.exists(label_path): return boxes\n",
    "    with open(label_path, 'r') as f:\n",
    "        for line in f:\n",
    "            _, x_center, y_center, width, height = map(float, line.strip().split())\n",
    "            w_abs, h_abs = width * img_width, height * img_height\n",
    "            xmin = (x_center * img_width) - (w_abs / 2)\n",
    "            ymin = (y_center * img_height) - (h_abs / 2)\n",
    "            boxes.append([xmin, ymin, xmin + w_abs, ymin + h_abs])\n",
    "    return boxes\n",
    "\n",
    "def evaluate_performance(predictions, ground_truth, iou_threshold=0.1):\n",
    "    \"\"\"\n",
    "    Evaluates detections against ground truth to calculate TP, FP, FN.\n",
    "    Returns a dictionary of counts and the list of matches.\n",
    "    \"\"\"\n",
    "    tp, fp = 0, 0\n",
    "    gt_matched = [False] * len(ground_truth)\n",
    "    matches = [] # List to store match status for each prediction\n",
    "\n",
    "    for pred_box in predictions:\n",
    "        best_iou, best_gt_idx = 0, -1\n",
    "        for i, gt_box in enumerate(ground_truth):\n",
    "            iou = calculate_iou(pred_box, gt_box)\n",
    "            if iou > best_iou:\n",
    "                best_iou, best_gt_idx = iou, i\n",
    "        \n",
    "        if best_iou > iou_threshold and best_gt_idx != -1 and not gt_matched[best_gt_idx]:\n",
    "            tp += 1\n",
    "            gt_matched[best_gt_idx] = True\n",
    "            matches.append({'box': pred_box, 'status': 'TP'})\n",
    "        else:\n",
    "            fp += 1\n",
    "            matches.append({'box': pred_box, 'status': 'FP'})\n",
    "    \n",
    "    fn = len(ground_truth) - sum(gt_matched)\n",
    "    \n",
    "    # Identify the specific FN boxes\n",
    "    unmatched_gt = [gt_boxes for i, gt_boxes in enumerate(ground_truth) if not gt_matched[i]]\n",
    "    for box in unmatched_gt:\n",
    "        matches.append({'box': box, 'status': 'FN'})\n",
    "\n",
    "    return {'tp': tp, 'fp': fp, 'fn': fn}, matches\n",
    "\n",
    "def draw_results_on_image(image, matches):\n",
    "    \"\"\"Draws colored bounding boxes on an image based on match status.\"\"\"\n",
    "    color_map = {\n",
    "        'TP': (0, 255, 0),   # Green\n",
    "        'FP': (0, 0, 255),   # Red\n",
    "        'FN': (255, 0, 0)    # Blue\n",
    "    }\n",
    "    for match in matches:\n",
    "        xmin, ymin, xmax, ymax = map(int, match['box'])\n",
    "        status = match['status']\n",
    "        cv2.rectangle(image, (xmin, ymin), (xmax, ymax), color_map[status], 2)\n",
    "    return image\n",
    "\n",
    "def classify_crop(crop_np, model, labels):\n",
    "    \"\"\"Classifies an image crop using the Teachable Machine model.\"\"\"\n",
    "    image = Image.fromarray(crop_np).resize((224, 224))\n",
    "    image_array = np.asarray(image)\n",
    "    normalized_image = (image_array.astype(np.float32) / 127.5) - 1\n",
    "    data = np.expand_dims(normalized_image, axis=0)\n",
    "    \n",
    "    prediction = model.predict(data, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    return labels[index].lower(), prediction[0][index]\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49389275",
   "metadata": {},
   "source": [
    "# Cell 4: Analysis on `land_dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5909d29b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Analyzing Optimal Threshold for 'land_dataset'</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b4065de839648b091841df4c92f2698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage 1: Pre-computing Detections:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-computation complete. Analyzed 465 YOLO detections from 100 images.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h4>Performance of Two-Stage Model vs. Classifier Threshold</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_e5e09_row0_col7 {\n",
       "  background-color: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_e5e09\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e5e09_level0_col0\" class=\"col_heading level0 col0\" >Threshold</th>\n",
       "      <th id=\"T_e5e09_level0_col1\" class=\"col_heading level0 col1\" >TP</th>\n",
       "      <th id=\"T_e5e09_level0_col2\" class=\"col_heading level0 col2\" >FP</th>\n",
       "      <th id=\"T_e5e09_level0_col3\" class=\"col_heading level0 col3\" >FN</th>\n",
       "      <th id=\"T_e5e09_level0_col4\" class=\"col_heading level0 col4\" >TN (Correct Rejections)</th>\n",
       "      <th id=\"T_e5e09_level0_col5\" class=\"col_heading level0 col5\" >Precision</th>\n",
       "      <th id=\"T_e5e09_level0_col6\" class=\"col_heading level0 col6\" >Recall</th>\n",
       "      <th id=\"T_e5e09_level0_col7\" class=\"col_heading level0 col7\" >F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e5e09_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_e5e09_row0_col0\" class=\"data row0 col0\" >0.50</td>\n",
       "      <td id=\"T_e5e09_row0_col1\" class=\"data row0 col1\" >350</td>\n",
       "      <td id=\"T_e5e09_row0_col2\" class=\"data row0 col2\" >115</td>\n",
       "      <td id=\"T_e5e09_row0_col3\" class=\"data row0 col3\" >118</td>\n",
       "      <td id=\"T_e5e09_row0_col4\" class=\"data row0 col4\" >0</td>\n",
       "      <td id=\"T_e5e09_row0_col5\" class=\"data row0 col5\" >75.27%</td>\n",
       "      <td id=\"T_e5e09_row0_col6\" class=\"data row0 col6\" >74.79%</td>\n",
       "      <td id=\"T_e5e09_row0_col7\" class=\"data row0 col7\" >0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e5e09_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_e5e09_row1_col0\" class=\"data row1 col0\" >0.55</td>\n",
       "      <td id=\"T_e5e09_row1_col1\" class=\"data row1 col1\" >346</td>\n",
       "      <td id=\"T_e5e09_row1_col2\" class=\"data row1 col2\" >109</td>\n",
       "      <td id=\"T_e5e09_row1_col3\" class=\"data row1 col3\" >122</td>\n",
       "      <td id=\"T_e5e09_row1_col4\" class=\"data row1 col4\" >6</td>\n",
       "      <td id=\"T_e5e09_row1_col5\" class=\"data row1 col5\" >76.04%</td>\n",
       "      <td id=\"T_e5e09_row1_col6\" class=\"data row1 col6\" >73.93%</td>\n",
       "      <td id=\"T_e5e09_row1_col7\" class=\"data row1 col7\" >0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e5e09_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_e5e09_row2_col0\" class=\"data row2 col0\" >0.60</td>\n",
       "      <td id=\"T_e5e09_row2_col1\" class=\"data row2 col1\" >340</td>\n",
       "      <td id=\"T_e5e09_row2_col2\" class=\"data row2 col2\" >109</td>\n",
       "      <td id=\"T_e5e09_row2_col3\" class=\"data row2 col3\" >128</td>\n",
       "      <td id=\"T_e5e09_row2_col4\" class=\"data row2 col4\" >6</td>\n",
       "      <td id=\"T_e5e09_row2_col5\" class=\"data row2 col5\" >75.72%</td>\n",
       "      <td id=\"T_e5e09_row2_col6\" class=\"data row2 col6\" >72.65%</td>\n",
       "      <td id=\"T_e5e09_row2_col7\" class=\"data row2 col7\" >0.742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e5e09_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_e5e09_row3_col0\" class=\"data row3 col0\" >0.65</td>\n",
       "      <td id=\"T_e5e09_row3_col1\" class=\"data row3 col1\" >331</td>\n",
       "      <td id=\"T_e5e09_row3_col2\" class=\"data row3 col2\" >107</td>\n",
       "      <td id=\"T_e5e09_row3_col3\" class=\"data row3 col3\" >137</td>\n",
       "      <td id=\"T_e5e09_row3_col4\" class=\"data row3 col4\" >8</td>\n",
       "      <td id=\"T_e5e09_row3_col5\" class=\"data row3 col5\" >75.57%</td>\n",
       "      <td id=\"T_e5e09_row3_col6\" class=\"data row3 col6\" >70.73%</td>\n",
       "      <td id=\"T_e5e09_row3_col7\" class=\"data row3 col7\" >0.731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e5e09_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_e5e09_row4_col0\" class=\"data row4 col0\" >0.70</td>\n",
       "      <td id=\"T_e5e09_row4_col1\" class=\"data row4 col1\" >323</td>\n",
       "      <td id=\"T_e5e09_row4_col2\" class=\"data row4 col2\" >106</td>\n",
       "      <td id=\"T_e5e09_row4_col3\" class=\"data row4 col3\" >145</td>\n",
       "      <td id=\"T_e5e09_row4_col4\" class=\"data row4 col4\" >9</td>\n",
       "      <td id=\"T_e5e09_row4_col5\" class=\"data row4 col5\" >75.29%</td>\n",
       "      <td id=\"T_e5e09_row4_col6\" class=\"data row4 col6\" >69.02%</td>\n",
       "      <td id=\"T_e5e09_row4_col7\" class=\"data row4 col7\" >0.720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e5e09_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_e5e09_row5_col0\" class=\"data row5 col0\" >0.75</td>\n",
       "      <td id=\"T_e5e09_row5_col1\" class=\"data row5 col1\" >315</td>\n",
       "      <td id=\"T_e5e09_row5_col2\" class=\"data row5 col2\" >97</td>\n",
       "      <td id=\"T_e5e09_row5_col3\" class=\"data row5 col3\" >153</td>\n",
       "      <td id=\"T_e5e09_row5_col4\" class=\"data row5 col4\" >18</td>\n",
       "      <td id=\"T_e5e09_row5_col5\" class=\"data row5 col5\" >76.46%</td>\n",
       "      <td id=\"T_e5e09_row5_col6\" class=\"data row5 col6\" >67.31%</td>\n",
       "      <td id=\"T_e5e09_row5_col7\" class=\"data row5 col7\" >0.716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e5e09_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_e5e09_row6_col0\" class=\"data row6 col0\" >0.80</td>\n",
       "      <td id=\"T_e5e09_row6_col1\" class=\"data row6 col1\" >305</td>\n",
       "      <td id=\"T_e5e09_row6_col2\" class=\"data row6 col2\" >94</td>\n",
       "      <td id=\"T_e5e09_row6_col3\" class=\"data row6 col3\" >163</td>\n",
       "      <td id=\"T_e5e09_row6_col4\" class=\"data row6 col4\" >21</td>\n",
       "      <td id=\"T_e5e09_row6_col5\" class=\"data row6 col5\" >76.44%</td>\n",
       "      <td id=\"T_e5e09_row6_col6\" class=\"data row6 col6\" >65.17%</td>\n",
       "      <td id=\"T_e5e09_row6_col7\" class=\"data row6 col7\" >0.704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e5e09_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_e5e09_row7_col0\" class=\"data row7 col0\" >0.85</td>\n",
       "      <td id=\"T_e5e09_row7_col1\" class=\"data row7 col1\" >298</td>\n",
       "      <td id=\"T_e5e09_row7_col2\" class=\"data row7 col2\" >83</td>\n",
       "      <td id=\"T_e5e09_row7_col3\" class=\"data row7 col3\" >170</td>\n",
       "      <td id=\"T_e5e09_row7_col4\" class=\"data row7 col4\" >32</td>\n",
       "      <td id=\"T_e5e09_row7_col5\" class=\"data row7 col5\" >78.22%</td>\n",
       "      <td id=\"T_e5e09_row7_col6\" class=\"data row7 col6\" >63.68%</td>\n",
       "      <td id=\"T_e5e09_row7_col7\" class=\"data row7 col7\" >0.702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e5e09_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_e5e09_row8_col0\" class=\"data row8 col0\" >0.90</td>\n",
       "      <td id=\"T_e5e09_row8_col1\" class=\"data row8 col1\" >278</td>\n",
       "      <td id=\"T_e5e09_row8_col2\" class=\"data row8 col2\" >74</td>\n",
       "      <td id=\"T_e5e09_row8_col3\" class=\"data row8 col3\" >190</td>\n",
       "      <td id=\"T_e5e09_row8_col4\" class=\"data row8 col4\" >41</td>\n",
       "      <td id=\"T_e5e09_row8_col5\" class=\"data row8 col5\" >78.98%</td>\n",
       "      <td id=\"T_e5e09_row8_col6\" class=\"data row8 col6\" >59.40%</td>\n",
       "      <td id=\"T_e5e09_row8_col7\" class=\"data row8 col7\" >0.678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e5e09_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_e5e09_row9_col0\" class=\"data row9 col0\" >0.95</td>\n",
       "      <td id=\"T_e5e09_row9_col1\" class=\"data row9 col1\" >264</td>\n",
       "      <td id=\"T_e5e09_row9_col2\" class=\"data row9 col2\" >61</td>\n",
       "      <td id=\"T_e5e09_row9_col3\" class=\"data row9 col3\" >204</td>\n",
       "      <td id=\"T_e5e09_row9_col4\" class=\"data row9 col4\" >54</td>\n",
       "      <td id=\"T_e5e09_row9_col5\" class=\"data row9 col5\" >81.23%</td>\n",
       "      <td id=\"T_e5e09_row9_col6\" class=\"data row9 col6\" >56.41%</td>\n",
       "      <td id=\"T_e5e09_row9_col7\" class=\"data row9 col7\" >0.666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x286367c55d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"border: 1px solid #ccc; padding: 10px; margin-top: 20px;\">\n",
       "        <h4>Baseline Performance (YOLOv5 Only)</h4>\n",
       "        <p>\n",
       "            <b>Precision:</b> 75.27% | \n",
       "            <b>Recall:</b> 74.79% | \n",
       "            <b>F1-Score:</b> 0.750\n",
       "        </p>\n",
       "        <p>(TP: 350, FP: 115, FN: 118)</p>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b1f32ad79c46a9bf2b469294c15186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x03\\xf6\\x00\\x00\\x02r\\x08\\x06\\x00\\x00\\x00nN[.\\x00\\x00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell to Find the Optimal Classifier Threshold\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "def find_optimal_threshold(dataset_name='land_dataset', num_images=100):\n",
    "    \"\"\"\n",
    "    Analyzes the two-stage model's performance across various classifier\n",
    "    confidence thresholds and compares it to the baseline YOLOv5 model.\n",
    "    \"\"\"\n",
    "    display(HTML(f\"<h3>Analyzing Optimal Threshold for '{dataset_name}'</h3>\"))\n",
    "    \n",
    "    # --- STAGE 1: Pre-computation of all detections and their scores ---\n",
    "    # This is done once to make the subsequent analysis much faster.\n",
    "    images_path = os.path.join(f'./{dataset_name}/', 'images')\n",
    "    labels_path = os.path.join(f'./{dataset_name}/', 'labels')\n",
    "    image_files = sorted(os.listdir(images_path))[:num_images]\n",
    "\n",
    "    all_detections = []\n",
    "    total_gt_boxes = 0\n",
    "    yolo_only_totals = {'tp': 0, 'fp': 0}\n",
    "\n",
    "    for image_name in tqdm(image_files, desc=\"Stage 1: Pre-computing Detections\"):\n",
    "        # Load data\n",
    "        image_path = os.path.join(images_path, image_name)\n",
    "        label_path = os.path.join(labels_path, os.path.splitext(image_name)[0] + '.txt')\n",
    "        \n",
    "        original_image_rgb = np.array(Image.open(image_path).convert(\"RGB\"))\n",
    "        h, w, _ = original_image_rgb.shape\n",
    "        gt_boxes = load_yolo_labels(label_path, w, h)\n",
    "        total_gt_boxes += len(gt_boxes)\n",
    "\n",
    "        # Get YOLOv5 predictions\n",
    "        yolo_results = yolo_model(original_image_rgb)\n",
    "        yolo_predictions = [pred[:4].tolist() for pred in yolo_results.xyxy[0].cpu().numpy()]\n",
    "        \n",
    "        # Evaluate YOLO-only performance for the baseline\n",
    "        gt_matched_yolo = [False] * len(gt_boxes)\n",
    "        for pred_box in yolo_predictions:\n",
    "            is_tp = False\n",
    "            for i, gt_box in enumerate(gt_boxes):\n",
    "                if not gt_matched_yolo[i] and calculate_iou(pred_box, gt_box) > 0.4:\n",
    "                    gt_matched_yolo[i] = True\n",
    "                    is_tp = True\n",
    "                    break\n",
    "            if is_tp:\n",
    "                yolo_only_totals['tp'] += 1\n",
    "            else:\n",
    "                yolo_only_totals['fp'] += 1\n",
    "\n",
    "            # Classify the crop for the two-stage analysis\n",
    "            xmin, ymin, xmax, ymax = map(int, pred_box)\n",
    "            crop = original_image_rgb[ymin:ymax, xmin:xmax]\n",
    "            if crop.size == 0: continue\n",
    "            \n",
    "            pred_class, confidence = classify_crop(crop, classifier_model, classifier_labels)\n",
    "            \n",
    "            all_detections.append({\n",
    "                \"class\": pred_class,\n",
    "                \"confidence\": confidence,\n",
    "                \"is_potential_tp\": is_tp \n",
    "            })\n",
    "    \n",
    "    yolo_only_totals['fn'] = total_gt_boxes - yolo_only_totals['tp']\n",
    "    print(f\"Pre-computation complete. Analyzed {len(all_detections)} YOLO detections from {num_images} images.\")\n",
    "\n",
    "    # --- STAGE 2: Evaluate performance across different thresholds ---\n",
    "    thresholds_to_test = np.arange(0.50, 1.0, 0.05) # Test from 0.50 to 0.95\n",
    "    analysis_results = []\n",
    "\n",
    "    for threshold in thresholds_to_test:\n",
    "        tp, fp, tn = 0, 0, 0\n",
    "        \n",
    "        for det in all_detections:\n",
    "            is_accepted = 'ship' in det['class'] and det['confidence'] >= threshold\n",
    "            \n",
    "            if is_accepted:\n",
    "                if det['is_potential_tp']:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    fp += 1\n",
    "            else: # If discarded by the classifier\n",
    "                if not det['is_potential_tp']: # It was a YOLO FP and we correctly discarded it\n",
    "                    tn += 1\n",
    "        \n",
    "        fn = total_gt_boxes - tp\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        analysis_results.append({\n",
    "            \"Threshold\": f\"{threshold:.2f}\",\n",
    "            \"TP\": tp, \"FP\": fp, \"FN\": fn, \"TN (Correct Rejections)\": tn,\n",
    "            \"Precision\": precision, \"Recall\": recall, \"F1-Score\": f1_score\n",
    "        })\n",
    "\n",
    "    # --- STAGE 3: Display Results ---\n",
    "    df_results = pd.DataFrame(analysis_results)\n",
    "    \n",
    "    # Find the best F1-score to highlight it\n",
    "    best_f1_score = df_results['F1-Score'].max()\n",
    "    \n",
    "    styled_df = df_results.style.apply(lambda s: ['background-color: yellow' if v == best_f1_score else '' for v in s], subset=['F1-Score'])\\\n",
    "                                .format({ \"Precision\": \"{:.2%}\", \"Recall\": \"{:.2%}\", \"F1-Score\": \"{:.3f}\" })\n",
    "\n",
    "    display(HTML(\"<h4>Performance of Two-Stage Model vs. Classifier Threshold</h4>\"))\n",
    "    display(styled_df)\n",
    "\n",
    "    # --- Baseline YOLOv5 Performance ---\n",
    "    yolo_precision = yolo_only_totals['tp'] / (yolo_only_totals['tp'] + yolo_only_totals['fp'])\n",
    "    yolo_recall = yolo_only_totals['tp'] / (yolo_only_totals['tp'] + yolo_only_totals['fn'])\n",
    "    yolo_f1 = 2 * (yolo_precision * yolo_recall) / (yolo_precision + yolo_recall)\n",
    "    \n",
    "    display(HTML(f\"\"\"\n",
    "    <div style=\"border: 1px solid #ccc; padding: 10px; margin-top: 20px;\">\n",
    "        <h4>Baseline Performance (YOLOv5 Only)</h4>\n",
    "        <p>\n",
    "            <b>Precision:</b> {yolo_precision:.2%} | \n",
    "            <b>Recall:</b> {yolo_recall:.2%} | \n",
    "            <b>F1-Score:</b> {yolo_f1:.3f}\n",
    "        </p>\n",
    "        <p>(TP: {yolo_only_totals['tp']}, FP: {yolo_only_totals['fp']}, FN: {yolo_only_totals['fn']})</p>\n",
    "    </div>\n",
    "    \"\"\"))\n",
    "\n",
    "    # --- Create and display the graph ---\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    ax.plot(df_results[\"Threshold\"], df_results[\"Precision\"], 'b-o', label='Precision')\n",
    "    ax.plot(df_results[\"Threshold\"], df_results[\"Recall\"], 'g-o', label='Recall')\n",
    "    ax.plot(df_results[\"Threshold\"], df_results[\"F1-Score\"], 'r-s', label='F1-Score', linewidth=3)\n",
    "    \n",
    "    # Add a line for the baseline F1-score for comparison\n",
    "    ax.axhline(y=yolo_f1, color='purple', linestyle=':', label=f'YOLOv5 Only F1-Score ({yolo_f1:.3f})')\n",
    "    \n",
    "    best_f1_row = df_results.loc[df_results['F1-Score'].idxmax()]\n",
    "    ax.axvline(x=best_f1_row['Threshold'], color='grey', linestyle='--', label=f'Best Two-Stage F1 ({best_f1_row[\"F1-Score\"]:.3f}) at Threshold {best_f1_row[\"Threshold\"]}')\n",
    "    \n",
    "    ax.set_title(f'Model Performance on \"{dataset_name}\"')\n",
    "    ax.set_xlabel('Classifier Confidence Threshold')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    \n",
    "    # Save the plot to a memory buffer and display as an image widget\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf, format='png', bbox_inches='tight')\n",
    "    buf.seek(0)\n",
    "    graph_widget = widgets.Image(value=buf.read(), format='png')\n",
    "    display(graph_widget)\n",
    "    buf.close()\n",
    "    plt.close(fig)\n",
    "\n",
    "# --- Run the analysis function ---\n",
    "find_optimal_threshold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f7943e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ed0fa30cb8c40c5af098a7f9efa6d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analyzing land_dataset:   0%|          | 0/562 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Performance on <u>land_dataset</u></h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YOLOv5 Only</th>\n",
       "      <th>YOLOv5 + Classifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True Positives (TP)</th>\n",
       "      <td>1465</td>\n",
       "      <td>1275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Positives (FP)</th>\n",
       "      <td>2404</td>\n",
       "      <td>1417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Negatives (FN)</th>\n",
       "      <td>2144</td>\n",
       "      <td>2334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>37.87%</td>\n",
       "      <td>47.36%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>40.59%</td>\n",
       "      <td>35.33%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-Score</th>\n",
       "      <td>0.392</td>\n",
       "      <td>0.405</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     YOLOv5 Only YOLOv5 + Classifier\n",
       "True Positives (TP)         1465                1275\n",
       "False Positives (FP)        2404                1417\n",
       "False Negatives (FN)        2144                2334\n",
       "Precision                 37.87%              47.36%\n",
       "Recall                    40.59%              35.33%\n",
       "F1-Score                   0.392               0.405"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run_full_analysis(dataset_name, yolo_model, classifier_model, classifier_labels):\n",
    "    \"\"\"\n",
    "    Runs the complete analysis for a given dataset and returns a results summary.\n",
    "    \"\"\"\n",
    "    # --- Setup paths and directories ---\n",
    "    DATASET_PATH = f'./{dataset_name}/'\n",
    "    IMAGES_PATH = os.path.join(DATASET_PATH, 'images')\n",
    "    LABELS_PATH = os.path.join(DATASET_PATH, 'labels')\n",
    "    \n",
    "    RESULTS_BASE_PATH = './results/'\n",
    "    YOLO_ONLY_RESULTS_PATH = os.path.join(RESULTS_BASE_PATH, f'{dataset_name}_yolo_only')\n",
    "    TWO_STAGE_RESULTS_PATH = os.path.join(RESULTS_BASE_PATH, f'{dataset_name}_two_stage')\n",
    "\n",
    "    for path in [YOLO_ONLY_RESULTS_PATH, TWO_STAGE_RESULTS_PATH]:\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "        os.makedirs(path)\n",
    "\n",
    "    image_files = sorted(os.listdir(IMAGES_PATH))\n",
    "    \n",
    "    # --- Initialize counters ---\n",
    "    yolo_only_totals = {'tp': 0, 'fp': 0, 'fn': 0}\n",
    "    two_stage_totals = {'tp': 0, 'fp': 0, 'fn': 0}\n",
    "\n",
    "    # --- Main processing loop ---\n",
    "    for image_name in tqdm(image_files, desc=f\"Analyzing {dataset_name}\"):\n",
    "        image_path = os.path.join(IMAGES_PATH, image_name)\n",
    "        label_path = os.path.join(LABELS_PATH, os.path.splitext(image_name)[0] + '.txt')\n",
    "        \n",
    "        original_image_bgr = cv2.imread(image_path)\n",
    "        original_image_rgb = cv2.cvtColor(original_image_bgr, cv2.COLOR_BGR2RGB)\n",
    "        h, w, _ = original_image_bgr.shape\n",
    "        \n",
    "        gt_boxes = load_yolo_labels(label_path, w, h)\n",
    "        yolo_results = yolo_model(original_image_rgb)\n",
    "        yolo_predictions = [pred[:4].tolist() for pred in yolo_results.xyxy[0].cpu().numpy()]\n",
    "\n",
    "        # 1. YOLO Only Analysis\n",
    "        yolo_perf, yolo_matches = evaluate_performance(yolo_predictions, gt_boxes)\n",
    "        for key in yolo_only_totals: yolo_only_totals[key] += yolo_perf[key]\n",
    "        \n",
    "        yolo_img_out = draw_results_on_image(original_image_bgr.copy(), yolo_matches)\n",
    "        cv2.imwrite(os.path.join(YOLO_ONLY_RESULTS_PATH, image_name), yolo_img_out)\n",
    "\n",
    "        # 2. Two-Stage Analysis (YOLO + Classifier)\n",
    "        filtered_predictions = []\n",
    "        for pred_box in yolo_predictions:\n",
    "            xmin, ymin, xmax, ymax = map(int, pred_box)\n",
    "            crop = original_image_rgb[ymin:ymax, xmin:xmax]\n",
    "            if crop.size == 0: continue\n",
    "            \n",
    "            pred_class, confidence = classify_crop(crop, classifier_model, classifier_labels)\n",
    "            # Assuming the positive class is the first one in labels.txt and contains 'ship'\n",
    "            if 'ship' in pred_class and confidence > 0.95:\n",
    "                filtered_predictions.append(pred_box)\n",
    "\n",
    "        two_stage_perf, two_stage_matches = evaluate_performance(filtered_predictions, gt_boxes)\n",
    "        for key in two_stage_totals: two_stage_totals[key] += two_stage_perf[key]\n",
    "        \n",
    "        two_stage_img_out = draw_results_on_image(original_image_bgr.copy(), two_stage_matches)\n",
    "        cv2.imwrite(os.path.join(TWO_STAGE_RESULTS_PATH, image_name), two_stage_img_out)\n",
    "\n",
    "    # --- Generate summary table ---\n",
    "    def calculate_metrics(results):\n",
    "        tp, fp, fn = results['tp'], results['fp'], results['fn']\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        return [tp, fp, fn, f\"{precision:.2%}\", f\"{recall:.2%}\", f\"{f1_score:.3f}\"]\n",
    "\n",
    "    yolo_metrics = calculate_metrics(yolo_only_totals)\n",
    "    two_stage_metrics = calculate_metrics(two_stage_totals)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'YOLOv5 Only': yolo_metrics,\n",
    "        'YOLOv5 + Classifier': two_stage_metrics\n",
    "    }, index=['True Positives (TP)', 'False Positives (FP)', 'False Negatives (FN)', 'Precision', 'Recall', 'F1-Score'])\n",
    "    \n",
    "    display(HTML(f\"<h3>Performance on <u>{dataset_name}</u></h3>\"))\n",
    "    display(df)\n",
    "\n",
    "# --- Run the analysis for the land dataset ---\n",
    "run_full_analysis('land_dataset', yolo_model, classifier_model, classifier_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4afd2a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<hr><h3>Debugging Image: P0001_1200_2000_10190_10990.jpg</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>YOLOv5 found 2 potential objects.</b> Now classifying each one:"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef6aa74ad004f29b760489284c67add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\x19\\x08\\x02\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b3eb92a38c64c218aee827b9ef2f41e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00%\\x00\\x00\\x004\\x08\\x02\\x00\\x00\\x00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr><h3>Debugging Image: P0001_1800_2600_7800_8600.jpg</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>YOLOv5 found 4 potential objects.</b> Now classifying each one:"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd30158233b4518952092c2e79c2361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x1d\\x00\\x00\\x00)\\x08\\x02\\x00\\x00\\…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "159a6307bc5b4a6cb992de16405b5462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x16\\x00\\x00\\x00,\\x08\\x02\\x00\\x00\\…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfcc8ac8068f49cf8859a8835c740c90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x07\\x00\\x00\\x00\\x0c\\x08\\x02\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e3e18c68f6f4d4fb177cbd7d059e579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x0f\\x00\\x00\\x00\\x05\\x08\\x02\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr><h3>Debugging Image: P0002_1800_2600_8400_9200.jpg</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>YOLOv5 found 3 potential objects.</b> Now classifying each one:"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21db29122ae742f6916b7eda19745079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00(\\x00\\x00\\x00!\\x08\\x02\\x00\\x00\\x00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e2cb4882f3940e180919d2790e59762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00A\\x00\\x00\\x004\\x08\\x02\\x00\\x00\\x00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d6a843639de4f8380fa0889ea79d954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x003\\x00\\x00\\x00\\x1f\\x08\\x02\\x00\\x00\\…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr><h3>Debugging Image: P0003_1200_2000_7800_8600.jpg</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>YOLOv5 found 2 potential objects.</b> Now classifying each one:"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bc1a445d64d4621bbd92b1e04620c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\r\\x00\\x00\\x00-\\x08\\x02\\x00\\x00\\x0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72cc5c2c1c54492caecdd2b69b2f0d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x0c\\x00\\x00\\x00\\x13\\x08\\x02\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr><h3>Debugging Image: P0003_600_1400_8400_9200.jpg</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>YOLOv5 found 4 potential objects.</b> Now classifying each one:"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c855809359644e0ba72c0d4fe1c3c965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00)\\x00\\x00\\x00$\\x08\\x02\\x00\\x00\\x00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd5102384d454c0a84a8eae6be2721da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x009\\x00\\x00\\x00\\x18\\x08\\x02\\x00\\x00\\…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5018b1d78d944755b56130f75ab2a831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x0f\\x00\\x00\\x00-\\x08\\x02\\x00\\x00\\…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a74b7c44f3a450eb94fdb8c0b5590f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\x0b\\x08\\x02\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr><h3>Debugging Image: P0004_1200_2000_7800_8600.jpg</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>YOLOv5 found 1 potential objects.</b> Now classifying each one:"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87af282cdd8242789979c4b1fb007e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x13\\x00\\x00\\x00\\x17\\x08\\x02\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr><h3>Debugging Image: P0004_1200_2000_8189_8989.jpg</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>YOLOv5 found 1 potential objects.</b> Now classifying each one:"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c929d9b4f64b67aea8ba5937cfe1ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x14\\x00\\x00\\x00\\x18\\x08\\x02\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr><h3>Debugging Image: P0004_600_1400_8189_8989.jpg</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>YOLOv5 found 5 potential objects.</b> Now classifying each one:"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14ab2fd89e9d4a1598019f2a230fedd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\"\\x00\\x00\\x00-\\x08\\x02\\x00\\x00\\x00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d048fce5b5034dcf8ec3d553f891ec2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00)\\x00\\x00\\x00(\\x08\\x02\\x00\\x00\\x00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339bac2faa9d4a10a744025468446679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00%\\x00\\x00\\x00.\\x08\\x02\\x00\\x00\\x00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d77630b8c7e4028a087691150dc74e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x14\\x00\\x00\\x001\\x08\\x02\\x00\\x00\\…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a047c83c33ef487d923c164c32ee4c21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x1e\\x00\\x00\\x00\\x1e\\x08\\x02\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr><h3>Debugging Image: P0005_0_800_7800_8600.jpg</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>YOLOv5 found 11 potential objects.</b> Now classifying each one:"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e71a90af2345b09341e3f01f19ca11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x15\\x00\\x00\\x00\\x14\\x08\\x02\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "775aaf613cf34fbdb0f88cb5ad45b243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x15\\x00\\x00\\x00\\x13\\x08\\x02\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bdc9e6a34f2487a8178882168e83ecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\r\\x00\\x00\\x00\\x13\\x08\\x02\\x00\\x00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2790977e8c043f2b34ebf7d19f5bfcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x1b\\x00\\x00\\x00\\x12\\x08\\x02\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb741709f8f4aca9f289935ad5ebb52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x0b\\x00\\x00\\x00\\x10\\x08\\x02\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "757ee1af8b774bb0b032f2781e7bda57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\x0e\\x08\\x02\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5be6d30b3c94c26b77257d96072f7d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x0b\\x00\\x00\\x00\\x0f\\x08\\x02\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4b83ebfabb149eea8b01936875e1bdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x15\\x00\\x00\\x00\\x0f\\x08\\x02\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b02d1145ed429f963f340a1cce3e19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x0c\\x00\\x00\\x00\\x10\\x08\\x02\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b891fb27ce584f7eb42182e4e8069101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x13\\x00\\x00\\x00\\x12\\x08\\x02\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d458bdb5d254d29811fabfc6bc20ca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x06\\x00\\x00\\x00\\n\\x08\\x02\\x00\\x00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr><h3>Debugging Image: P0005_1800_2600_8189_8989.jpg</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>YOLOv5 found 1 potential objects.</b> Now classifying each one:"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7764146e1a9745819975b45058de65bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x1f\\x00\\x00\\x000\\x08\\x02\\x00\\x00\\…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell for Classifier Debugging (CORRECTED)\n",
    "\n",
    "# --- Configuration for the debug cell ---\n",
    "from IPython.display import display, HTML\n",
    "import ipywidgets as widgets\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "NUM_IMAGES_TO_DEBUG = 10\n",
    "DATASET_TO_DEBUG = 'land_dataset'  # You can change this to 'sea_dataset'\n",
    "DEBUG_CONFIDENCE_THRESHOLD = 0.50\n",
    "POSITIVE_CLASS_NAME = 'ship'\n",
    "\n",
    "# --- Debugging Logic ---\n",
    "# This code assumes the models and helper functions from previous cells are already loaded in memory.\n",
    "images_path = os.path.join(f'./{DATASET_TO_DEBUG}/', 'images')\n",
    "image_files = sorted(os.listdir(images_path))[:NUM_IMAGES_TO_DEBUG]\n",
    "\n",
    "# Check if models are loaded to avoid errors\n",
    "if 'yolo_model' not in locals() or 'classifier_model' not in locals():\n",
    "    print(\"❌ Error: Models are not loaded. Please run the model loading cell (Cell 2) first.\")\n",
    "else:\n",
    "    for image_name in image_files:\n",
    "        display(HTML(f\"<hr><h3>Debugging Image: {image_name}</h3>\"))\n",
    "        image_path = os.path.join(images_path, image_name)\n",
    "        \n",
    "        # Load image\n",
    "        original_image_rgb = np.array(Image.open(image_path).convert(\"RGB\"))\n",
    "        \n",
    "        # Get YOLO predictions\n",
    "        yolo_results = yolo_model(original_image_rgb)\n",
    "        yolo_predictions = [pred[:4].tolist() for pred in yolo_results.xyxy[0].cpu().numpy()]\n",
    "        \n",
    "        display(HTML(f\"<b>YOLOv5 found {len(yolo_predictions)} potential objects.</b> Now classifying each one:\"))\n",
    "        \n",
    "        if not yolo_predictions:\n",
    "            display(HTML(\"<p>No objects to classify.</p>\"))\n",
    "            continue\n",
    "\n",
    "        # Classify and display each crop\n",
    "        for i, pred_box in enumerate(yolo_predictions):\n",
    "            xmin, ymin, xmax, ymax = map(int, pred_box)\n",
    "            crop = original_image_rgb[ymin:ymax, xmin:xmax]\n",
    "\n",
    "            if crop.size == 0: continue\n",
    "\n",
    "            # --- LINEA CORREGIDA ---\n",
    "            # Changed cv2.COLOR_RGB_BGR to cv2.COLOR_RGB2BGR\n",
    "            _, buffer = cv2.imencode('.png', cv2.cvtColor(crop, cv2.COLOR_RGB2BGR))\n",
    "            crop_widget = widgets.Image(value=buffer.tobytes(), format='png', width=200)\n",
    "\n",
    "            # Get the classification result from the helper function\n",
    "            pred_class, confidence = classify_crop(crop, classifier_model, classifier_labels)\n",
    "\n",
    "            # --- Decision Logic ---\n",
    "            is_kept = POSITIVE_CLASS_NAME in pred_class and confidence >= DEBUG_CONFIDENCE_THRESHOLD\n",
    "            decision_text = \"KEPT\" if is_kept else \"DISCARDED\"\n",
    "            color = \"green\" if is_kept else \"red\"\n",
    "\n",
    "            # Create the HTML widget\n",
    "            result_html_string = f\"\"\"\n",
    "            <div style='padding-left: 10px; font-size: 14px; border-left: 3px solid #ccc; margin-left: 5px;'>\n",
    "                <p><b>Detection #{i+1}</b></p>\n",
    "                <p>Classifier Result: <b>{pred_class.title()}</b></p>\n",
    "                <p>Confidence: <b>{confidence:.2%}</b></p>\n",
    "                <p>Decision: <b style='color:{color};'>{decision_text}</b></p>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "            html_widget = widgets.HTML(value=result_html_string)\n",
    "            \n",
    "            # Create and display the HBox\n",
    "            hbox_container = widgets.HBox([crop_widget, html_widget])\n",
    "            display(hbox_container)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
